\addcontentsline{toc}{chapter}{Занятие 7. Гауссовые системы}
\chapter*{Занятие 7. Гауссовые системы}

\addcontentsline{toc}{section}{Контрольные вопросы и задания}
\section*{Контрольные вопросы и задания}

\subsubsection*{Приведите определение гауссового случайного вектора,
                ковариационной матрицы гауссового случаайного вектора.}

$ \vec{ \xi } = \left( \xi_1, \dotsc, \xi_n \right) $ --- гауссовый, если
$$ \forall \alpha_1, \dotsc, \alpha_n: \,
  \sum \limits_{k = 1}^n \alpha_k \xi_k$$
--- гауссовская случайная величина.

$A = cov_{ \vec{ \xi } \vec{ \xi }}, \,
  A_{ij} =
  cov \left( \xi_i, \xi_j \right) =
  M \left( \xi_i \xi_j \right) - M \xi_i M \xi_j$.

\subsubsection*{Какими свойствами владеет ковариационная матрица?}

$A_{ii} = D \xi_i \geq 0$ --- на диагонале --- неотрицательные числа.

Матрица симметрична: $A_{ij} = A_{ji}$.

Матрица неотрицательно определена.

\subsubsection*{Как изменяются характеристики гауссового случайного
                вектора при действии на него линейного оператора?}
Пускай $ \vec{ \xi }$ случайный $n$-элементный вектор,
имеющий гауссовское распределение с параметрами $ \vec{a}$ и
$A, \,
  \vec{ \xi } \sim N \left( \vec{a}, A \right), \,
  \vec{b} \in \mathbb{R}^m, \,
  T \in \mathbb{R}^{m \times n}$.
Тогда $T \vec{ \xi } + \vec{b} \sim N \left( T \vec{a} + \vec{b}, TAT^* \right) $.

\subsubsection*{При каких условиях гауссовский случайный вектор имеет плотность распределения?}

Плотность можно записать только в случае, когда $det \, A \neq 0$.

\subsubsection*{Запишите плотность распределения гауссового случайного вектора.}

$$p_{ \vec{ \xi }} \left( \vec{x} \right) =
  \left( \frac{1}{ \sqrt{2 \pi }} \right)^n \cdot \frac{1}{ \sqrt{det \, A}} \cdot
  e^{- \frac{1}{2} \left[ \vec{x} - \vec{a}, A^{-1} \left( \vec{x} - \vec{a} \right) \right] }.$$

\subsubsection*{Сформулируйте теорему про нормальную корреляцию.}

Есть гауссоский вектор $ \vec{ \xi } \circ \vec{ \eta }$ с ненулевой ковариацией
$cov_{ \vec{ \xi } \circ \vec{ \eta }, \vec{ \xi } \circ \vec{ \eta }} \neq 0$.

Определитель ковариационной матрицы вектора $ \eta $ положителен
$$det \, cov_{ \vec{ \eta }, \vec{ \eta }} \geq
  0.$$

Тогда вектор $ \vec{ \xi }$ при условии $ \vec{ \eta }$ ---
гауссовский случайный вектор
$$ \left. \vec{ \xi} \right| \vec{ \eta } \sim
  N \left( \vec{m}, D \right).$$

Параметры $ \vec{m}$ и $D$ имеют следующий вид
$$ \vec{m} =
  M \vec{ \xi } +
  cov_{ \vec{ \xi }, \vec{ \eta }}
    cov_{ \vec{ \eta }, \vec{ \eta }^{-1}} \left( \eta - M \vec{ \eta } \right), \,
  D =
  cov_{ \vec{ \xi }, \vec{ \xi }} -
  cov_{ \vec{ \xi }, \vec{ \eta }} cov_{ \vec{ \eta }, \vec{ \eta }}^{-1}
    cov_{ \vec{ \eta }, \vec{ \xi }}.$$

\addcontentsline{toc}{section}{Аудиторные задачи}
\section*{Аудиторные задачи}

\subsubsection*{7.3}

\textit{Задание.}
Может ли матрица $A$ быть ковариационной матрицей гауссового случайного вектора, если:
\begin{enumerate}[label=\alph*)]
  \item $A =
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 0 & 0
    \end{bmatrix};$
  \item $A =
    \begin{bmatrix}
      1 & 2 \\
      3 & 9
    \end{bmatrix};$
  \item $A =
    \begin{bmatrix}
      4 & 2 \\
      2 & -1
    \end{bmatrix};$
  \item $A =
    \begin{bmatrix}
      1 & 2 \\
      2 & 5
    \end{bmatrix};$
  \item $A =
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 6 & -1 \\
      1 & -1 & 12
    \end{bmatrix};$
  \item $A =
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 10
    \end{bmatrix}.$
\end{enumerate}
Если так, то предъявите такой вектор.

\textit{Решение.}
\begin{enumerate}[label=\alph*)]
  \item Нет (матрица не квадратная);
  \item нет (матрица не симметрична);
  \item нет (на диагонали отрицательные числа);
  \item матрица квадратная, симметричная, неотрицательно определённая, на диагонали ---
  неотрицательные числа;
  \item матрица квадратная, симметричная, на диагонали --- неотрицательные числа.
  Первый минор $M_1 = 1 \geq 0$, второй минор $M_2 = 6 - 4 = 2 \geq 0$, третий минор
  $$det \left(
      \begin{bmatrix}
        1 & 2 & 1 \\
        2 & 6 & -1 \\
        1 & -1 & 12
      \end{bmatrix}
    \right) =
    1 \cdot \left( 72 - 1 \right) - 2 \cdot \left( 24 + 1 \right) + 1 \cdot \left( -2 - 6 \right) =
    13 \geq
    0,$$
  значит, матрица неотрицательно определена,
  то есть может быть ковариационной матрицей гауссового случайного вектора;
  \item матрица квадратная, симметричная, на диагонали --- неотрицательные чиста.
  Первый минор $M_1 = 1 \geq 0$, второй минор $M_2 = 5 - 2 = 3 \geq 0$,
  третий минор
  $$M_3 =
    1 \cdot \left( 50 - 0 \right) - 2 \cdot \left( 20 - 0 \right) + 1 \cdot \left( 0 - 5 \right) =
    50 - 40 - 5 =
    5 \geq
    0,$$
  значит, матрица неотрицательно определена,
  то есть может быть ковариационной матрицей гауссового случайного вектора.
\end{enumerate}

\subsubsection*{7.5}

\textit{Задание.}
Пусть $ \xi = \left( \xi_1, \xi_2, \xi_3 \right) $ ---
гауссовый вектор с математическим ожиданием $ \left( -1, 0, 2 \right) $
и матрицей ковариаций из задачи 7.3 e).
\begin{enumerate}[label=\alph*)]
  \item Выпишите плотность распределения и характеристическую фукнцию для
  $ \xi_1, \,
    \left( \xi_1, \xi_2 \right), \,
    \xi $.
  \item Найдите матрицу ковариаций и математическое ожидание для вектора
  $ \left( \eta_1, \eta_2, \eta_3 \right) $,
  где $ \eta_1 = \xi_1 - \xi_2, \, \eta_2 = \xi_1 + 2 \xi_2 + 3 \xi_3, \, \eta_3 = \xi_3$.
  \item Вычислите $M \eta_2^2, \, M \eta_2^3, \, M \eta_2^4$.
  \item Выясните, являются ли случайные величины $ \eta_1$ и $ \eta_2$ независимыми.
  \item Найдите
  $M \left( \xi \; \middle| \; \left( \xi_1, \xi_3 \right) \right), \,
    M \left( \xi_1 \; \middle| \; \xi_2 \right) $.
\end{enumerate}

\textit{Решение.}
$$A =
  \begin{bmatrix}
    1 & 2 & 1 \\
    2 & 6 & -1 \\
    1 & -1 & 12
  \end{bmatrix}$$

\begin{enumerate}[label=\alph*)]
  \item Начинаем со случайной величины $ \xi_1$.
  Запишем её как линейную комбинацию гауссового вектора
  $ \xi_1 =
    1 \cdot \xi_1 + 0 \cdot \xi_2 + 0 \cdot \xi_3$.
  Следовательно, $ \xi_1 \sim N \left( -1, 1 \right) $, где $-1$ --- это первый элемент $ \vec{a}$,
  а 1 --- элемент $a_{11}$ матрицы ковариации.

  Записываем плотность
  $$p_{ \xi_1} \left( x \right) =
    \frac{1}{ \sqrt{2 \pi }} \cdot e^{- \frac{ \left( x + 1 \right)^2}{2}}$$
  и характеристическую функцию $ \varphi_{ \xi_1} \left( t \right) = e^{-it - \frac{t^2}{2}}$.

  Переходим к вектору $ \left( \xi_1, \xi_2 \right) $.
  Это гауссовый вектор,
  потому что произвольная линейная комбинация
  $ \lambda_1 \xi_1 + \lambda_2 \xi_2 = \lambda_1 \xi_1 + \lambda_2 \xi_2 + 0 \cdot \xi_3$ ---
  это случайная величина, которая должна иметь нормальное распределение.
  Отсюда следует, что $ \left( \xi_1, \xi_2 \right) = \vec{ \xi }$ --- гауссовый вектор.

  $M \vec{ \xi } = \left( -1, 0 \right) $, а ковариационная матрица данного вектора ---
  это сужение исходной ковариационной матрицы на соответствующие вектора
  $$R =
    \begin{bmatrix}
      1 & 2 \\
      2 & 6
    \end{bmatrix}.$$

  Записываем характеристическую функцию и плотность.
  Матрица коавриации невырождена, так что плотность можем записать
  $$p_{ \left( \xi_1, \xi_2 \right) } \left( \vec{x} \right) =
    \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}} \sqrt{2}} \cdot
    e^{- \frac{1}{2} \left( x_1 + 1 \, x_2 \right) \cdot
      \begin{bmatrix}
        3 & -1 \\
        -1 & \frac{1}{2}
      \end{bmatrix} \cdot
      \begin{bmatrix}
        x_1 + 1 \\
        x_2
      \end{bmatrix}
    }.$$
  Определитель этой матрицы $det \, R = 6 - 4 = 2$.
  Обратная матрица
  $$R^{-1} =
    \frac{1}{2} \cdot
    \begin{bmatrix}
      6 & -2 \\
      -2 & 1
    \end{bmatrix} =
    \begin{bmatrix}
      3 & -1 \\
      -1 & \frac{1}{2}
    \end{bmatrix}.$$
  Подставим полученное в выражение для плотности
  \begin{equation*}
    \begin{split}
      \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}} \sqrt{2}} \cdot
      e^{- \frac{1}{2} \cdot
        \begin{bmatrix}
          x_1 + 1 & x_2
        \end{bmatrix} \cdot
        \begin{bmatrix}
          3 & -1 \\
          -1 & \frac{1}{2}
        \end{bmatrix} \cdot
        \begin{bmatrix}
          x_1 + 1 \\
          x_2
        \end{bmatrix}
      } = \\
      = \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}} \sqrt{2}} \cdot
      exp \left\{
        - \frac{1}{4} \cdot
        \begin{bmatrix}
          x_1 + 6 - 2 \cdot x_2 & -2x_1 - 2 + 2x_2
        \end{bmatrix} \cdot
        \begin{bmatrix}
          x_1 + 1 \\
          x_2
        \end{bmatrix}
      \right\} = \\
      = \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}} \sqrt{2}} \cdot
      exp \left\{
        - \frac{1}{4} \cdot
        \left[
          6 \, x_1 \left( x_1 + 1 \right) - 2x_1 \left( x_1 - 1 \right) - 2x_1 x_2 + 2x_2^2
        \right]
      \right\}.
    \end{split}
  \end{equation*}

  Для вектора $ \xi $ математическое ожидание равно $ \left( -1, 0, 2 \right) $,
  а матрица ковариаций совпадает с исходной;
  \item задача: выписать матрицу,
  с помощью которой получим вектор
  $$ \vec{ \eta } =
    \left( \eta_1, \eta_2, \eta_3 \right).$$
  В условии задачи дано, что
  $$A_{ \xi } =
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 6 & -1 \\
      1 & -1 & 12
    \end{bmatrix},$$
  а $ \vec{m}_{ \xi } = \left( -1, 0, 2 \right) $.

  Посмотрим, с помощью какой матрицы получаются преобразования.
  $$ \begin{bmatrix}
      \eta_1 \\
      \eta_2 \\
      \eta_3
    \end{bmatrix} =
    \begin{bmatrix}
      1 & -1 & 0 \\
      1 & 2 & 3 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \xi_1 \\
      \xi_2 \\
      \xi_3
    \end{bmatrix}.$$

  Это матрица $B$.
  Находим математическое ожидание вектора $ \eta $.
  Получаем
  $$ \vec{m}_{ \eta } =
    B \vec{m}_{ \xi } =
    \begin{bmatrix}
      1 & -1 & 0 \\
      1 & 2 & 3 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 \\
      0 \\
      2
    \end{bmatrix} =
    \begin{bmatrix}
      -1 \\
      5 \\
      2
    \end{bmatrix}.$$

  Теперь ищем матрицу ковариации
  $$A_{ \eta } =
    BA_{ \xi } B^T =
    \begin{bmatrix}
      1 & -1 & 0 \\
      1 & 2 & 3 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 6 & -1 \\
      1 & -1 & 12
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1 & 0 \\
      -1 & 2 & 0 \\
      0 & 3 & 1
    \end{bmatrix}.$$
  Перемножим две последние матрицы
  $$ \begin{bmatrix}
      1 & -1 & 0 \\
      1 & 2 & 3 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 6 & -1 \\
      1 & -1 & 12
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1 & 0 \\
      -1 & 2 & 0 \\
      0 & 3 & 1
    \end{bmatrix} =
    \begin{bmatrix}
      1 & -1 & 0 \\
      1 & 2 & 3 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 & 8 & 1 \\
      -4 & 11 & -1 \\
      2 & 35 & 12
    \end{bmatrix}.$$
  Перемножим матрицы
  $$ \begin{bmatrix}
      1 & -1 & 0 \\
      1 & 2 & 3 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 & 8 & 1 \\
      -4 & 11 & -1 \\
      2 & 35 & 12
    \end{bmatrix} =
    \begin{bmatrix}
      -3 & -3 & 2 \\
      -3 & 135 & 35 \\
      2 & 35 & 12
    \end{bmatrix}.$$

  Эта матрица ковариаций, значит, она симметричная;
  \item случайная величина $ \eta_2$ имеет распределение Гаусса, как компонент гауссового вектора.
  Нужно указать параметры, то есть её математическое ожидание и дисперсию.
  $ \eta_2 \sim N \left( 5, 135 \right) $.
  У нас нецентрированная случайная величина.
  Значит, $ \eta_2 - 5 \sim N \left( 0, 135 \right) $.
  Поэтому $M \left( \eta_2 - 5 \right)^2 = 135$, потому что это её дисперсия.
  Раскрываем слева скобки,
  пользуемся линейностью математического ожидания и находим второй момент
  $M \eta_2^2 - 25M \eta_2 + 25 =
    135$.
  Математическое ожидание $ \eta_2$ мы знаем,
  выражаем второй момент $M \eta_2^2 = 10 \cdot 5 + 110 = 160$.

  Третий момент этой случайной величины равен $M \left( \eta_2 - 5 \right)^3 = 0$.
  Так же раскрываем скобки слева $M \eta_2^3 - 15M \eta_2^2 + 75M \eta_2 - 125 = 0$,
  откуда $M \eta_2^3 = 15 \cdot 160 - 75 \cdot 5 + 125 = 2400 - 250 = 2150$.

  Четвёртый момент находится аналогично;
  \item если бы они были независимыми, ковариация была бы равна нулю.
  Это элемент матрицы $ \left( 1, 2 \right) $.
  Он равен $Cov \left( \eta_1, \eta_2 \right) = -3 \neq 0$.
  Отсюда следует, что случайные величины зависимы.

  Компопненты гауссового вектора независимы тогда и только тогда, когда они некоррелируемы;
  \item пользуемся теоремой о нормальной корреляции.

  Записываем, как в теореме,
  \begin{equation*}
    \begin{split}
      M \left[ \xi_1 \; \middle| \; \left( \xi_2, \xi_3 \right) \right] = \\
      = M \xi_1 +
      Cov \left[ \xi_1, \left( \xi_2, \xi_3 \right) \right] \cdot
      \left\{
        Cov \left[ \left( \xi_2, \xi_3 \right), \left( \xi_2, \xi_3 \right) \right]
      \right\}^{-1} \left[ \left( \xi_2, \xi_3 \right) - M \left( \xi_2, \xi_3 \right) \right]^T.
    \end{split}
  \end{equation*}
  Подставляем
  \begin{equation*}
    \begin{split}
      M \xi_1 +
      Cov \left[ \xi_1, \left( \xi_2, \xi_3 \right) \right] \cdot
      \left\{
        Cov \left[ \left( \xi_2, \xi_3 \right), \left( \xi_2, \xi_3 \right) \right]
      \right\}^{-1}
      \left[ \left( \xi_2, \xi_3 \right) - M \left( \xi_2, \xi_3 \right) \right]^T = \\
      = -1 +
      \begin{bmatrix}
        2 & 1
      \end{bmatrix}
      \begin{bmatrix}
        6 & -1 \\
        -1 & 12
      \end{bmatrix}^{-1}
      \begin{bmatrix}
        \xi_2 - 0 \\
        \xi_3 - 2
      \end{bmatrix} = \\
      = -1 +
      \begin{bmatrix}
        2 & 1
      \end{bmatrix} \cdot \frac{1}{71}
      \begin{bmatrix}
        12 & 1 \\
        1 & 6
      \end{bmatrix}
      \begin{bmatrix}
        \xi_2 \\
        \xi_3 - 2
      \end{bmatrix} =
      -1 +
      \begin{bmatrix}
        25 & 8
      \end{bmatrix} \cdot \frac{1}{71}
      \begin{bmatrix}
        \xi_2 \\
        \xi_3 - 2
      \end{bmatrix} = \\
      = -1 + \frac{1}{71} \left( 25 \xi_2 + 8 \xi_3 - 16 \right)
    \end{split}
  \end{equation*}
  --- оценка $ \xi_1$ по вектору $ \left( \xi_2, \xi_3 \right)$.

  Аналогично
  $$M \left( \xi_1 \; \middle| \; \xi_2 \right) =
    M \xi_1 +
    Cov \left( \xi_1, \xi_2 \right) \left[ Cov \left( \xi_2, \xi_2 \right) \right]^{-1} \cdot
    \left( \xi_2 - M \xi_2 \right).$$
  Подставляем известные значения
  $$M \xi_1 +
    Cov \left( \xi_1, \xi_2 \right) \left[ Cov \left( \xi_2, \xi_2 \right) \right]^{-1} \cdot
    \left( \xi_2 - M \xi_2 \right) =
    -1 + 2 \cdot \frac{1}{6} \left( \xi_2 - 0 \right).$$
  Сократим
  $$-1 + 2 \cdot \frac{1}{6} \left( \xi_2 - 0 \right) =
    -1 + \frac{1}{3} \cdot \xi_2.$$
\end{enumerate}

\subsubsection*{7.6}

\textit{Задание.}
Пусть $ \left( \xi_1, \xi_2 \right) $ ---
гауссовый вектор с вектором средних $ \left( 1, -1 \right) $
и ковариационной матрицей из примера задачи 7.3 d).
Найдите такое $c$, что случайные величины $ \xi_1$ и $ \xi_2 + c \xi_1$ являются независимыми.

\textit{Решение.}
$$ \vec{ \xi } = \left( \xi_1, \xi_2 \right), \,
  vec{m}_{ \xi } = \left(1, -1 \right), \,
  A =
  \begin{bmatrix}
    1 & 2 \\
    2 & 5
  \end{bmatrix}.$$

Хотим найти $c$, при котором $ \xi_1$ и $ \xi_2 + c \xi_1$ --- независимы.
Сначала нужно показать, что $ \left( \xi_1, \xi_2 + c \xi_1 \right) $ является гауссовым вектором,
чтобы применить теорему.
Берём какую-то комбинацию и группируем
$$ \lambda_1 \xi_1 + \lambda_2 \left( \xi_2 + c \xi_1 \right) =
  \xi_1 \left( \lambda_1 + \lambda_2 c \right) + \xi_2 \lambda_2.$$
Видим, что это линейная комбинация координат гауссового вектора,
которая имеет нормальное распределение.
Значит, веткор гауссовый.
Тогда независимость координат эквивалентна их некоррелируемости
$$Cov \left( \xi_1, \xi_2 + c \xi_1 \right) =
  0.$$
Воспользуемся линейностью ковариации
$$Cov \left( \xi_1, \xi_2 + c \xi_1 \right) =
  Cov \left( \xi_1, \xi_2 \right) + c \cdot Cov \left( \xi_1, \xi_1 \right) =
  2 + c \cdot 1 =
  2 + c =
  0,$$
следовательно, $c = -2$.

\subsubsection{7.7}

\textit{Задание.}
Случайные величины $ \xi_1, \xi_2, \xi_3$
являются независимыми и имеют стандартное нормальное распределение каждая.
Найдите:
\begin{enumerate}[label=\alph*)]
  \item распределние вектора $ \left( \xi_2 - \xi_1, \xi_3 - \xi_1 \right) $;
  \item плотность $p \left( x_1, x_2 \right) $ распределения вектора
  $ \left( \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \,
    \xi_{ \left( 3 \right) } - \xi_{ \left( 1 \right) } \right) $.
\end{enumerate}

\textit{Решение.} $ \xi_1, \xi_2, \xi_3 \sim N \left( 0, 1 \right) $.
\begin{enumerate}[label=\alph*)]
  \item Вектор $ \vec{ \eta } = \left( \xi_2 - \xi_1, \xi_3 - \xi_1 \right) $.

  Если они независимы, то  их ковариация равна нулю.

  Сформируем вектор $ \left( \xi_1, \xi_2, \xi_3 \right) $ и покажем, что это гауссовский вектор
  $$ \forall \lambda_1, \lambda_2, \lambda_3 \, \sum \limits_{i = 1}^3 \lambda_i \xi_i \sim
    N \left( 0, \lambda_1^2 + \lambda_2^2 + \lambda_3^2 \right).$$
  Значит, вектор $ \left( \xi_1, \xi_2, \xi_3 \right) $ является гауссовским.
  Поэтому вектор $ \vec{ \eta }$ --- гауссовский.
  Покажем это.
  $$ \forall \lambda_1, \lambda_2, \,
    \lambda_1 \xi_2 - \lambda_1 \xi_1 + \lambda_2 \xi_3 - \lambda_2 \xi_1 =
    \xi_1 \left( - \lambda_1 - \lambda_2 \right) + \lambda_1 \xi_2 + \lambda_2 \xi_3.$$
  Видим, что это линейная комбинация координат гауссового вектора, значит,
  вектор действительно гауссовый.
  Чтобы указать распределние, нужны параметры,
  то есть вектор математических ожиданий и матрица ковариации
  $$ \vec{ \eta } = B \vec{ \xi } =
    \begin{bmatrix}
      \xi_2 - \xi_1 \\
      \xi_3 - \xi_1
    \end{bmatrix} =
    \begin{bmatrix}
      -1 & 1 & 0 \\
      -1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \xi_1 \\
      \xi_2 \\
      \xi_3
    \end{bmatrix},$$
  где
  $$ \begin{bmatrix}
      -1 & 1 & 0 \\
      -1 & 0 & 1
    \end{bmatrix} =
    B.$$

  Матрицей $B$ действуем на вектор математических ожиданий и на ковариационную матрицу
  $ \vec{m}_{ \eta } = B \vec{m}_{ \xi } = 0, \,
    A_{ \eta } = BA_{ \xi } B^T$,
  где $A_{ \xi }$ --- единичная матрица, потому что они все независимы, некоррелируемы, значит,
  $$BA_{ \xi } B^T =
    BB^T =
    \begin{bmatrix}
      -1 & 1 & 0 \\
      -1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 & -1 \\
      1 & 0 \\
      0 & 1
    \end{bmatrix}.$$
  Должна быть симметричная матрица
  $$ \begin{bmatrix}
      -1 & 1 & 0 \\
      -1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 & -1 \\
      1 & 0 \\
      0 & 1
    \end{bmatrix} =
    \begin{bmatrix}
      2 & 1 \\
      1 & 2
    \end{bmatrix}$$
  --- матрица ковариации;
  \item скобками обозначена упорядоченность.
  $ \vec{ \eta } =
    \left( \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \,
    \xi_{ \left( 3 \right) } - \xi_{ \left( 1 \right) } \right) $.

  Если бы знали плотность распределения
  $ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) $
  (знаем из задачи 1.10)
  $p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
    \left( x_1, x_2, x_3 \right) $, можем сказать,
    что вектор $ \vec{ \eta }$ получается линейным преобразованием с помощью матрицы $B$.

  Формула замены переменных
  $$p_{B \vec{ \xi }} \left( \vec{x} \right) =
    \frac{1}{det \, B} \cdot p_{ \vec{ \xi }} \left( B^{-1} \vec{x} \right).$$
  Значит, матрица $B$ должна быть только квадратной.
  Найдём плотность вектора
  $ \left(
      \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \,
      \xi_{ \left( 3 \right) } - \xi_{ \left( 1 \right) }, \,
      \xi_{ \left( 1 \right) }
    \right) \,
  p \left( u_1, u_2, u_3 \right) $.

Далее проинтегрируем по $u_3$.
Это делают, когда размерность не совпадает.

$p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) } =
  3! p_{ \left( \xi_1, \xi_2, \xi_3 \right) } \left( x_1, x_2, x_3 \right) \cdot
  \mathbbm{1} \left\{ x_1 < x_2 , x_3 \right\} $.
Запишем для нашего случая.
$ \xi_1, \xi_2, \xi_3$ --- независимы, значит, плотность вектора --- это произведение плотностей
\begin{equation*}
  \begin{split}
    3! p_{ \left( \xi_1, \xi_2, \xi_3 \right) } \left( x_1, x_2, x_3 \right) \cdot
    \mathbbm{1} \left\{ x_1 < x_2 , x_3 \right\} = \\
    = 6p_{ \xi_1} \left( x_1 \right) p_{ \xi_2} \left( x_2 \right) p_{ \xi_3} \left( x_3 \right) \cdot
    \mathbbm{1} \left\{ x_1 < x_2 < x_3 \right\} = \\
    = 6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
    e^{- \frac{1}{2} \left( x_1^2 + x_2^2 + x_3^2 \right) } \cdot
    \mathbbm{1} \left\{ x_1 < x_2 < x_3 \right\}.
  \end{split}
\end{equation*}

Записываем преобразование
$$ \begin{bmatrix}
    \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) } \\
    \xi_{ \left( 3 \right) } - \xi_{ \left( 1 \right) } \\
    \xi_{ \left( 1 \right) }
  \end{bmatrix} =
  \begin{bmatrix}
    -1 & 1 & 0 \\
    -1 & 0 & 1 \\
    1 & 0 & 0
  \end{bmatrix}
  \begin{bmatrix}
    \xi_{ \left( 1 \right) } \\
    \xi_{ \left( 2 \right) } \\
    \xi_{ \left( 3 \right) }
  \end{bmatrix}.$$

  Значит, матрица
  $$B =
  \begin{bmatrix}
    -1 & 1 & 0 \\
    -1 & 0 & 1 \\
    1 & 0 & 0
  \end{bmatrix}.$$

  Чтобы применить формулу замены переменных, нужны определитель и обратная матрица.
  Раскладываем по последней строке
  $$det \, B = 1, \,
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 1 \\
      0 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 3 \right) } - \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 1 \right) }
    \end{bmatrix} =
    \begin{bmatrix}
      \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 2 \right) } \\
      \xi_{ \left( 3 \right) }
    \end{bmatrix}.$$
  Значит,
  $$B^{-1} =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 1 \\
      0 & 1 & 1
    \end{bmatrix}.$$

  Действуем матрицей на вектор
  $$B^{-1} \vec{x} =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 1 \\
      0 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\
      x_2 \\
      x_3
    \end{bmatrix} =
    \begin{bmatrix}
      x_3 \\
      x_1 + x_3 \\
      x_2 + x_3
    \end{bmatrix}.$$
  Значит,
  $$p_{B \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
    \left( \vec{x} \right) =
    \frac{1}{det \, B} \cdot
    p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
    \left( B^{-1} \vec{x} \right).$$
  Подставляем значение плотности
  \begin{equation*}
    \begin{split}
      \frac{1}{det \, B} \cdot
      p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
      \left( B^{-1} \vec{x} \right) = \\
      = 6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left[ x_3^2 + \left( x_1 + x_3 \right)^2 + \left( x_2 + x_3 \right)^2 \right] } \cdot
      \mathbbm{1} \left\{ x_3 < x_1 + x_3 < x_2 + x_3 \right\}.
    \end{split}
  \end{equation*}
  Можем везде отнять $x_3$.
  Получим
  \begin{equation*}
    \begin{split}
      6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left[ x_3^2 + \left( x_1 + x_3 \right)^2 + \left( x_2 + x_3 \right)^2 \right] } \cdot
      \mathbbm{1} \left\{ x_3 < x_1 + x_3 < x_2 + x_3 \right\} = \\
      = 6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left[ x_3^2 + \left( x_1 + x_3 \right)^2 + \left( x_2 + x_3 \right)^2 \right] } \cdot
      \mathbbm{1} \left\{ 0 < x_1 < x_2 \right\}.
    \end{split}
  \end{equation*}
  Нужно проинтегрировать по $x_3$, то есть выделить полный квадрат по $x_3$.

  Получаем
  $$3x_32 + 2x_1 x_3 + 2x_2 x_3 + x_1^2 + x_2^2 =
    \left( \sqrt{3} x_3 + \frac{x_1 + x_2}{ \sqrt{3}} \right)^2 + x_1^2 + x_2^2 -
    \left( \frac{x_1 + x_2}{ \sqrt{3}} \right)^3.$$

  Нужная плотность вектора
  \begin{equation*}
    \begin{split}
      p_{ \left( \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \xi_{ \left( 3 \right) } - \xi_{ \left( 1 \right) } \right) }
      \left( x_1, x_2 \right) = \\
      = \frac{6}{2 \pi } \cdot
      e^{- \frac{1}{2} \left[ x_1^2 + x_2^2 - \frac{ \left( x_1 + x_2 \right)^2}{3} \right] } \cdot
      \mathbbm{1} \left\{ 0 < x_1 < x_2 \right\} \times \\
      \times \int \limits_{ \mathbb{R}}
        \frac{1}{ \sqrt{2 \pi }} \cdot
        e^{- \frac{1}{2} \left[ \sqrt{3} x_3 + \frac{ \left( x_1 + x_2 \right)^2}{3} \right]}
      dx_3 =
      \frac{6}{2 \sqrt{3} \pi } \cdot
      e^{- \frac{1}{2} \left[ x_1^2 + x_2^2 - \frac{ \left( x_1 + x_2 \right)^2}{3} \right] }.
    \end{split}
  \end{equation*}
\end{enumerate}

\subsubsection{7.8}

\textit{Задание.}
Пусть $ \xi $ и $ \eta $ --- независимые стандартные гауссовские случайные величины.
Докажите,
что величины $ \xi + \eta $ и $ \xi - \eta $
являются независимыми гауссовскими случайными величинами.

\textit{Решение.} $ \xi, \eta \sim N \left(0, 1 \right) $ --- независимы.
Хотим показать, что $ \xi + \eta $ и $ \xi - \eta $ --- независимые.
Стандартный способ --- через характеристические функции.
Формируем вектор $ \left( \xi + \eta, \xi - \eta \right)$.
Должно быть
$$ \varphi_{ \left( \xi + \eta, \xi - \eta \right) } \left( t_1, t_2 \right) =
  \varphi_{ \xi + \eta } \left( t_1 \right) \cdot \varphi_{ \xi - \eta } \left( t_2 \right) $$
тогда и только тогда, когда $ \xi - \eta, \, \xi + \eta $ --- независимы --- критерий.

Заметим, что раз $ \xi, \eta $ --- независимы, то вектор $ \left( \xi, \eta \right) $ ---
гауссовский.
Можно убедиться, что $ \left( \xi + \eta, \xi - \eta \right) $ --- гауссовский.
Тогда достаточно показать, что $Cov \left( \xi + \eta, \xi - \eta \right) = 0$.

Случайная величина $ \xi + \eta $ имеет распределение $N \left( 0, 2 \right) $.

Её характеристическая функция равна $ \varphi_{ \xi + \eta } \left( t_1 \right) = e^{-t_1^2}$.

Случайная величина $ \xi - \eta $ имеет распределение $N \left( 0, 2 \right) $,
потому что <<минус>> из дисперсии выносится с квадратом.

Она имеет характеристическую функцию $ \varphi_{ \xi - \eta } \left( t_2 \right) = e^{-t_2^2}$.

Значит,
произведение
$ \varphi_{ \xi + \eta } \left( t_1 \right) \varphi_{ \xi - \eta } \left( t_2 \right) =
  e^{- \left( t_1^2 + t_2^2 \right) }$.

Переходим к характеристической функции вектора.
По опрделению
$$ \varphi_{ \left( \xi + \eta, \xi - \eta \right) } \left( t_1, t_2 \right) =
  Me^{i \left[ t_1 \left( \xi + \eta \right) + t_2 \left( \xi - \eta \right) \right] }.$$
Группируем $ \xi $ и $ \eta $,
получаем
$$Me^{i \left[ t_1 \left( \xi + \eta \right) + t_2 \left( \xi - \eta \right) \right] } =
  Me^{i \left( t_1 \xi + t_1 \eta + t_2 \xi - t_2 \eta \right) } =
  Me^{i \left[ \xi \left( t_1 + t_2 \right) + \eta \left( t_1 - t_2 \right) \right] }.$$
Случайные величины $ \xi $ и $ \eta $ --- независимы, значит,
$$Me^{i \left[ \xi \left( t_1 + t_2 \right) + \eta \left( t_1 - t_2 \right) \right] } =
  \varphi_{ \xi } \left( t_1 + t_2 \right) \varphi_{ \eta} \left( t_1 - t_2 \right).$$
Подставляем явный вид.
Это стандартное нормальное распределение, следовательно,
$ \varphi_{ \xi } \left( t_1 + t_2 \right) \varphi_{ \eta} \left( t_1 - t_2 \right) =
  e^{- \frac{ \left( t_1 + t_2 \right)^2}{2}} e^{- \frac{ \left( t_1 - t_2 \right)^2}{2}} =
  e^{- \left( t_1^2 + t_2^2 \right) }$.

Случайный вектор $ \left( \xi, \eta \right) $ --- гауссовкий,
потому что
$$ \forall \alpha, \beta \,
  \alpha \xi + \beta \eta \sim N \left( 0, \alpha^2 + \beta^2 \right) $$
--- нормальное распределение.
Отсюда следует, что $ \left( \xi + \eta, \xi - \eta \right) $ --- гауссовский,
потому что
$$ \forall \alpha, \beta \,
  \alpha \left( \xi + \eta \right) + \beta \left( \xi - \eta \right) =
  \alpha \xi + \alpha \eta + \beta \xi - \beta \eta =
  \left( \alpha + \beta \right) \xi + \left( \alpha - \beta \right) \eta $$
--- линейная комбинация элементов гауссовского вектора,
или в явном виде
$N \left( 0, \left( \alpha + \beta \right)^2 + \left( \alpha - \beta \right)^2 \right) $.
Они будет независимыми, если некоррелируемы.

Из линейности следует,
что
$$Cov \left( \xi + \eta, \xi - \eta \right) =
  Cov \left( \xi, \xi \right) - Cov \left( \xi, \eta \right) +
  Cov \left( \eta, \xi \right) - Cov \left( \eta, \eta \right).$$
Случайные величины $ \xi $ и $ \eta $ --- независимые, и тем более некоррелируемы,
$Cov \left( \xi, \xi \right) $ --- это её дисперсия, значит,
$Cov \left( \eta, \xi \right) - Cov \left( \eta, \eta \right) = 1 - 1 = 0$, следовательно,
они некоррелируемые и независимые.

\subsubsection*{7.9}

\textit{Задание.}
Пусть положительная случайная величина $R$ имеет распределение Рэлея с плотностью распределения:
$$f_R \left( r \right) =
  \frac{r}{ \sigma^2} \cdot e^{- \frac{r^2}{2 \sigma^2}}, \,
  r > 0,$$
где $ \sigma^2 > 0$, и пусть $ \theta $ ---
независимая от неё случайная величина с равномерным распределением на $ \left( 0, 2 \pi \right) $.
Докажите,
что величины $X = R \cos \theta $ и $Y = R \sin \theta $
являются независимыми и имеют нормальное расределение $N \left( 0, \sigma^2 \right) $.

\textit{Решение.}
Пусть есть вектор $ \vec{ \xi } = \left( \xi_1, \dotsc, \xi_n \right) $ и допустим,
что $p_{ \vec{ \xi }} \left( x_1, \dotsc, x_n \right) $ --- известно.

$ \vec{ \eta } =
  \varphi \left( \vec{ \xi } \right) $.

Будет замена
$$ \begin{cases}
    z_1 = \varphi_1 \left( x_1, \dotsc, x_n \right), \\
    z_2 = \varphi \left( x_1, \dotsc, x_n \right), \\
    \dotsc \\
    x_n = \varphi \left( x_1, \dotsc, x_n \right).
  \end{cases}$$

Тогда
$$p_{ \vec{ \eta }} \left( \vec{z} \right) =
  p_{ \vec{ \eta }} \left( z_1, \dotsc, z_n \right) =
  \frac{1}{J \left( z \right) } \cdot
  p_{ \vec{ \xi }} \left( \varphi^{-1} \left( \vec{x} \right) \right),$$
где $ \varphi^{-1} \left( \vec{z} \right) $ --- обратное отображение, а
$$J \left( x \right) =
  \begin{vmatrix}
    \frac{ \partial \varphi_1}{ \partial x_1} & \dotsc & \frac{ \partial \varphi_1}{ \partial x_n} \\
    \dotsc & \dotsc & \dotsc \\
    \frac{ \partial \varphi_n}{ \partial x_1} & \dotsc & \frac{ \partial \varphi_n}{ \partial x_n}
  \end{vmatrix}$$
--- определитель Якобиана.

Сформируем вектор $ \left( R, \theta \right) $.

Запишем плотность этого вектора $p_{ \left( R, \theta \right) } \left( x, y \right) $.

Рассматриваем замену переменных.

Нужно перевести
$ \left( R, \theta \right) \to \left( \varphi \right)
  \left( R \cos \theta, R \sin \theta \right) $.

Имеем замену
$$ \begin{cases}
    z_1 = x \cos y, \\
    z_2 = x \sin y.
  \end{cases}$$

Такая замена координат приводит к такому преобразованию.
После этого согласно с формулой замены переменных нужно записать плотность нового вектора.

$ \theta \sim U \left( \left[ 0, 2 \pi \right] \right) $.

Случайные величины $R, \theta $ --- независимы между собой.
Запишем плотность вектора как произведение плотностей,
потому что они независимы
$p_{ \left( R, \theta \right) } \left( x, y \right) =
  p_R \left( x \right) p_{ \theta } \left( y \right) $.
Подставляем явный вид, включая индикаторы
$$p_R \left( x \right) p_{ \theta } \left( y \right) =
  \frac{x}{ \sigma^2} \cdot e^{- \frac{x^2}{2 \sigma^2}} \cdot
  \mathbbm{1} \left\{ x > 0 \right\} \cdot \frac{1}{2 \pi } \cdot
  \mathbbm{1} \left\{ y \in \left[ 0, 2 \pi \right] \right\}.$$

Записываем
$$J \left( z \right) =
  \begin{vmatrix}
    \frac{ \partial z_1}{ \partial x} & \frac{ \partial z_1}{ \partial y} \\
    \frac{ \partial z_1}{ \partial x} & \frac{ \partial z_2}{ \partial y}
  \end{vmatrix} =
  \begin{vmatrix}
    \cos y & -x \sin y \\
    \sin y & x \cos y
  \end{vmatrix} =
  x \cos^2 y + x \sin^2 y =
  x =
  \sqrt{z_1^2 + z_2^2}.$$

По сути это полярная замена координат.

Пишем плотность в новых переменных, индикаторы никаких ограничений на $z_1, z_2$ не дают
$$p_{ \left( R \cos \theta, R \sin \theta \right) } \left( z_1, z_2 \right) =
  \frac{1}{ \sqrt{z_1^2 + z_2^2}} \cdot \frac{ \sqrt{z_1^2 + z_2^2}}{ \sigma^2} \cdot
  e^{- \left( z_1^2 + x_2^2 \right) \cdot \frac{1}{2 \sigma^2}} \cdot \frac{1}{2 \pi } =
  \frac{1}{2 \pi \sigma^2} \cdot e^{- \frac{z_1^2 + z_2^2}{2 \sigma^2}}.$$
Это похоже на гауссовское распределение
$$ \frac{1}{2 \pi \sigma^2} \cdot e^{- \frac{z_1^2 + z_2^2}{2 \sigma^2}} =
  \left( \frac{1}{ \sqrt{2 \pi } \sigma } \cdot e^{- \frac{z_1^2}{2 \sigma^2}} \right) \cdot
  \left( \frac{1}{ \sqrt{2 \pi } \sigma } \cdot e^{- \frac{z_2^2}{2 \sigma^2}} \right) =
  p_{R \cos \theta } \left( z_1 \right) p_{R \sin \theta } \left( z_2 \right).$$
Из этого следует независимость.

\addcontentsline{toc}{section}{Домашнее задание}
\section*{Домашнее задание}

\subsubsection*{7.11}

\textit{Задание.}
Может ли матрица $A$ быть ковариационной матрицей гауссовского случайного вектора, если:
\begin{enumerate}[label=\alph*)]
  \item $$A =
    \begin{bmatrix}
      1 & 1 & 0 \\
      1 & 1 & 0 \\
      0 & 0 & 2
    \end{bmatrix};$$
  \item $$A =
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 5
    \end{bmatrix}.$$
\end{enumerate}

\textit{Решение.}
\begin{enumerate}[label=\alph*)]
  \item Матрица квадратная, симметричная, на диагонали --- неотрицательные числа.
  Первый минор $M_1 = 1 \geq 0$, второй минор
  $$M_2 =
    \begin{vmatrix}
      1 & 1 \\
      1 & 1
    \end{vmatrix} =
    1 - 1 =
    0 \geq 0,$$
  третий минор
  $$M_3 =
    \begin{vmatrix}
      1 & 1 & 0 \\
      1 & 1 & 0 \\
      0 & 0 & 2
    \end{vmatrix} =
    0 \cdot
    \begin{vmatrix}
      1 & 1 \\
      0 & 0
    \end{vmatrix} - 0 \cdot
    \begin{vmatrix}
      1 & 1 \\
      0 & 0
    \end{vmatrix} + 2 \cdot
    \begin{vmatrix}
      1 & 1 \\
      1 & 1
    \end{vmatrix} =
    0 \geq
    0,$$
  значит матрица неотрицательно определённная,
  то есть может быть ковариационной матрицей гауссовского случайного вектора;
  \item матрица квадратная, симметричная, на диагонали --- неотрицательные числа.
  Первый минор $M_1 = 1 \geq 0$, второй минор
  $$M_2 =
    \begin{vmatrix}
      1 & 2 \\
      2 & 5
    \end{vmatrix} =
    5 - 4 =
    1 \geq
    0,$$
  третий минор
  $$M_3 =
    \begin{vmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 5
    \end{vmatrix} =
    1 \cdot
    \begin{vmatrix}
      2 & 5 \\
      1 & 0
    \end{vmatrix} - 0 \cdot
    \begin{vmatrix}
      1 & 2 \\
      1 & 0
    \end{vmatrix} + 5 \cdot
    \begin{vmatrix}
      1 & 2 \\
      2 & 5
    \end{vmatrix} =
    -5 + 5 \left( 5 - 4 \right) =
    0,$$
  значит матрица неотрицательно определённая,
  то есть может быть ковариационной матрицей гауссовского случайного вектора.
\end{enumerate}

\subsubsection*{7.12}

\textit{Задание.}
Пусть $ \left( \xi_1, \xi_2, \xi_3 \right) $ ---
гауссовский вектор со средним $ \left( 1, 2, 5 \right) $ и матрицей ковариаций из задачи 7.3 f).
\begin{enumerate}[label=\alph*)]
  \item Запишите плотность распределения и характеристическую функцию для вектора
  $ \left( \xi_1, \xi_2, \xi_3 \right) $.
  \item Найдите матрицу ковариаций и среднее для вектора $ \left( \eta_1, \eta_2, \eta_3 \right) $,
  где $ \eta_1 = \xi_1 + \xi_2, \, \eta_2 = \xi_2 + \xi_3, \, \eta_3 = \xi_1 + \xi_3$.
  \item Выясните, являются ли случайные величины $ \eta_2, \eta_3$ независимыми.
  \item Найдите условные математические ожидания
  $$M \left( \xi_2 \; \middle| \; \left( \xi_1, \xi_3 \right) \right), \,
    M \left( \xi_1 \; \middle| \; \xi_2 \right).$$
  \item Вычислите $M \eta_1^2, M \eta_1^3, M \eta_1^4$.
\end{enumerate}

\textit{Решение.}
\begin{enumerate}[label=\alph*)]
  \item $ \left( \xi_1, \xi_2, \xi_3 \right) = \vec{ \xi }$ ---
  гауссовский вектор с математическим ожиданием $M \vec{ \xi } = \left( 1, 2, 5 \right) $
  и ковариационной матрицей
  $$A =
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 10
    \end{bmatrix}.$$

  Записываем характеристическую функцию и плотность.

  $$det \, A =
    \begin{vmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 10
    \end{vmatrix} =
    1 \cdot
    \begin{vmatrix}
      2 & 5 \\
      1 & 0
    \end{vmatrix} - 0 \cdot
    \begin{vmatrix}
      1 & 2 \\
      1 & 0
    \end{vmatrix} + 10 \cdot
    \begin{vmatrix}
      1 & 2 \\
      2 & 5
    \end{vmatrix} =
    -5 + 10 \left( 5 - 4 \right) =
    5.$$
  Матрица ковариаций невырождена, так что может записать
  $$p_{ \left( \xi_1, \xi_2, \xi_3 \right) } \left( \vec{x} \right) =
    \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}}} \cdot \frac{1}{ \sqrt{5}} \cdot
    e^{- \frac{1}{2} \cdot
      \begin{bmatrix}
        x_1 - 1 & x_2 - 2 & x_3 - 5
      \end{bmatrix} \cdot A^{-1} \cdot
      \begin{bmatrix}
        x_1 - 1 \\
        x_2 - 2 \\
        x_3 - 5
      \end{bmatrix}}.$$
  Найдём обратную матрицу к матрице ковариаций
  $$A^{-1} =
    \frac{1}{5} \cdot
    \begin{bmatrix}
      50 & -10 & -5 \\
      -10 & 9 & 2 \\
      -5 & 2 & 1
    \end{bmatrix}.$$
  Перемножив вектора и матрицы в степени экспоненты, получим
  \begin{equation*}
    \begin{split}
      \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}}} \cdot \frac{1}{ \sqrt{5}} \cdot
      e^{- \frac{1}{2} \cdot
        \begin{bmatrix}
          x_1 - 1 & x_2 - 2 & x_3 - 5
        \end{bmatrix} \cdot A^{-1} \cdot
        \begin{bmatrix}
          x_1 - 1 \\
          x_2 - 2 \\
          x_3 - 5
        \end{bmatrix}} = \\
      = \frac{1}{ \left( 2 \pi \right)^{ \frac{3}{2}} \sqrt{5}} \cdot
      e^{50x_1^2 + 9x_2^2 + x_3^2 - 40x_1 x_2 - 10x_1 x_3 + 4x_2 x_3 + 30x_1 - 16x_2 - 8x_3 + 21}.
    \end{split}
  \end{equation*}

  Запишем характеристическую функцию
  $$ \varphi_{ \vec{ \xi }} \left( \vec{ \lambda } \right) =
    exp \left\{
      i \left( \vec{ \lambda }, M \vec{ \xi } \right) -
      \frac{1}{2} \cdot \left( A \vec{ \lambda }, \vec{ \lambda }
    \right) \right\};$$
  \item задача: выписать матрицу,
  с помощью которой получается вектор
  $$ \vec{ \eta } =
    \left( \eta_1, \eta_2, \eta_3 \right).$$

  В условии задачи дано, что
  $$A_{ \xi } =
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 10
    \end{bmatrix},$$
  а $ \vec{m}_{ \xi } = \left( 1, 2, 5 \right) $.

  Посмотрим, с помощью какой матрицы получается преобразование
  $$ \begin{bmatrix}
      \eta_1 \\
      \eta_2 \\
      \eta_3
    \end{bmatrix} =
    \begin{bmatrix}
      1 & 1 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \xi_1 \\
      \xi_2 \\
      \xi_3
    \end{bmatrix}.$$

  Это матрица $B$.
  Находим математическое ожидание вектора $ \eta $.
  Получаем
  $$ \vec{m}_{ \eta } =
    B \vec{m}_{ \xi } =
    \begin{bmatrix}
      1 & 1 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 \\
      2 \\
      5
    \end{bmatrix} =
    \begin{bmatrix}
      1 + 2 \\
      2 + 5 \\
      1 + 5
    \end{bmatrix}
    \begin{bmatrix}
      3 \\
      7 \\
      6
    \end{bmatrix}.$$

  Теперь ищем матрицу ковариаций
  $$A_{ \eta } =
    BA_{ \xi } B^T =
    \begin{bmatrix}
      1 & 1 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 10
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 1 \\
      1 & 1 & 0 \\
      0 & 1 & 1
    \end{bmatrix}.$$
  Перемножим две последние матрицы
  $$ \begin{bmatrix}
      1 & 1 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 1 \\
      2 & 5 & 0 \\
      1 & 0 & 10
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 1 \\
      1 & 1 & 0 \\
      0 & 1 & 1
    \end{bmatrix} =
    \begin{bmatrix}
      1 & 1 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      3 & 3 & 2 \\
      7 & 5 & 2 \\
      1 & 10 & 11
    \end{bmatrix}.$$
  Перемножаем матрицы
  $$ \begin{bmatrix}
      1 & 1 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      3 & 3 & 2 \\
      7 & 5 & 2 \\
      1 & 10 & 11
    \end{bmatrix} =
    \begin{bmatrix}
      10 & 8 & 4 \\
      8 & 15 & 13 \\
      4 & 13 & 13
    \end{bmatrix}.$$

  Это матрица ковариаций, значит, она симметрична;
  \item если бы они были независимыми, ковариация была бы равна нулю.

  Это элемент матрицы $ \left( 2, 3 \right) $.
  Он равен $Cov \left( \eta_2, \eta_3 \right) = 13 \neq 0$.
  Отсюда следует, что случайные величины зависимы.

  Компоненты гауссовского вектора независимы тогда и только тогда, когда они некоррелированные;
  \item пользуемся теоремой о нормальной корреляции.

  Записываем, как в теореме,
  \begin{equation*}
    \begin{split}
      M \left[ \xi_2 \; \middle| \; \left( \xi_1, \xi_3 \right) \right] = \\
      = M \xi_2 + \\
      + Cov \left[ \xi_2, \left( \xi_1, \xi_3 \right) \right] \cdot
      \left\{
        Cov \left[ \left( \xi_1, \xi_3 \right), \left( \xi_1, \xi_3 \right) \right]
      \right\}^{-1} \cdot
      \left[ \left( \xi_1, \xi_3 \right) - M \left( \xi_1, \xi_3 \right) \right]^T.
    \end{split}
  \end{equation*}
  Подставим
  \begin{equation*}
    \begin{split}
      M \xi_2 +
      Cov \left[ \xi_2, \left( \xi_1, \xi_3 \right) \right] \cdot
      \left\{
        Cov \left[ \left( \xi_1, \xi_3 \right), \left( \xi_1, \xi_3 \right) \right]
      \right\}^{-1} \cdot
      \left[ \left( \xi_1, \xi_3 \right) - M \left( \xi_1, \xi_3 \right) \right]^T = \\
      = 7 +
      \begin{bmatrix}
        2 & 0
      \end{bmatrix}
      \begin{bmatrix}
        1 & 1 \\
        1 & 10
      \end{bmatrix}^{-1}
      \begin{bmatrix}
        \xi_1 - 1 \\
        \xi_3 - 5
      \end{bmatrix} =
      7 +
      \begin{bmatrix}
        2 & 0
      \end{bmatrix} \cdot \frac{1}{9}
      \begin{bmatrix}
        10 & -1 \\
        -1 & 1
      \end{bmatrix}
      \begin{bmatrix}
        \xi_1 - 1 \\
        \xi_3 - 5
      \end{bmatrix} = \\
      = 7 +
      \begin{bmatrix}
        20 & -2
      \end{bmatrix} \cdot \frac{1}{9}
      \begin{bmatrix}
        \xi_1 - 1 \\
        \xi_3 - 5
      \end{bmatrix} =
      7 + \frac{1}{9} \left( 20 \xi_1 - 20 - 2 \xi_3 + 10 \right) = \\
      = 7 + \frac{1}{9} \left( 20 \xi_1 - 2 \xi_2 - 10 \right)
    \end{split}
  \end{equation*}
  --- оценка $ \xi_2$ по вектору $ \left( \xi_1, \xi_3 \right) $.

  Аналогично
  $M \left( \xi_1 \; \middle| \; \xi_2 \right) =
    M \xi_1 + Cov \left( \xi_1, \xi_2 \right) \cdot
    \left[ Cov \left( \xi_2, \xi_2 \right) \right]^{-1} \left( \xi_2 - M \xi_2 \right).$
  Подставим известные значения
  $$M \xi_1 + Cov \left( \xi_1, \xi_2 \right) \cdot
    \left[ Cov \left( \xi_2, \xi_2 \right) \right]^{-1} \left( \xi_2 - M \xi_2 \right) =
    1 + 2 \cdot \frac{1}{5} \left( \xi_2 - 2 \right).$$
  Сократим
  $$1 + 2 \cdot \frac{1}{5} \left( \xi_2 - 2 \right) =
    1 + \frac{2}{5} \cdot \xi_2 - \frac{4}{5} =
    \frac{1}{5} - \frac{2}{5} \cdot \xi_2;$$
  \item случайная величина $ \eta_1$ имеет распределение Гаусса,
  как компонента гауссовского вектора.
  Нужно указать параметры, то есть её математическое ожидание и дисперсию
  $ \eta_1 \sim N \left( 3, 10 \right) $.
  У нас смещённая случайная величина.
  Значит, $ \eta_1 - 3 \sim N \left( 0, 10 \right) $.
  Поэтому
  $$M \left( \eta_1 - 3 \right)^2 =
    10,$$
  потому что это её дисперсия.
  Раскроем слева скобки,
  пользуемся линейностью математического ожидания и находим второй момент
  $$M \eta_1^2 - 6M \eta_1 + 9 =
    10.$$
  Математическое ожидание $ \eta_1$ мы знаем,
  выражаем второй момент $M \eta_1^2 = 6 \cdot 3 - 9 + 10 = 18 + 1 = 19$.

  Третий момент этой случайной величины $M \left( \eta_1 - 3 \right)^3 = 0$.

  Так же раскрываем скобки слева $M \eta_1^3 - 3M \eta_1^2 \cdot 3 + 3M \eta_1 \cdot 9 - 27 = 0$,
  откуда $M \eta_3^2 = 9 \cdot 19 - 27 \cdot 3 + 27 = 117$.

  Четвёртый момент находим аналогично
  $$M \left( \eta_1 - 3 \right)^4 =
    M \eta_1^4 - 4M \eta_1^3 \cdot 3 + 6 \cdot M \eta_1^2 \cdot 9 - 4M \eta_1 \cdot 27 + 81.$$
  Подставляем известные значения моментов
  $$M \eta_1^4 - 4M \eta_1^3 \cdot 3 + 6 \cdot M \eta_1^2 \cdot 9 - 4M \eta_1 \cdot 27 + 81 =
    M \eta_1^4 - 12 \cdot 117 + 54 \cdot 19 - 108 \cdot 3 + 81.$$
  Умножаем и складываем константы
  $$M \eta_1^4 - 12 \cdot 117 + 54 \cdot 19 - 108 \cdot 3 + 81 =
    M \eta_1^4 - 621 =
    10^2 \cdot 3! =
    100 \cdot 6 =
    600,$$
  откуда $M \eta_1^4 = 600 + 621 = 1221$.
\end{enumerate}

\subsubsection*{7.13}

\textit{Задание.}
Случайные величины $ \xi_1, \xi_2, \xi_3$
являются независимыми и имеют стандартное нормальное распределние каждая.
Найдите:
\begin{enumerate}[label=\alph*)]
  \item распределение вектора $ \left( \xi_2 - \xi_1, \xi_3 - \xi_2 \right) $;
  \item плотность $p \left( x_1, x_2 \right) $ распределения вектора
  $ \left( \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \,
    \xi_{ \left( 3 \right) } - \xi_{ \left( 2 \right) } \right) $.
\end{enumerate}

\textit{Решение.}
\begin{enumerate}[label=\alph*)]
  \item Вектор $ \vec{ \eta } = \left( \xi_2 - \xi_1, \xi_3 - \xi_2 \right) $.

  Если случайные величины независимы, то их ковариация равна нулю.

  Сформируем вектор $ \left( \xi_1, \xi_2, \xi_3 \right) $ и покажем, что это гауссовский вектор
  $$ \forall \lambda_1, \lambda_2, \lambda_3 \,
    \sum \limits_{i = 1}^3 \lambda_1 \xi_i \sim
    N \left( 0, \lambda_1^2 + \lambda_2^2 + \lambda_3^2 \right).$$

  Значит, вектор $ \left( \xi_1, \xi_2, \xi_3 \right) $ является гауссовским.
  Поэтому вектор $ \vec{ \eta } $ --- гауссовский.

  Покажем это
  $$ \forall \lambda_1, \lambda_2 \,
    \lambda_1 \xi_2 - \lambda_1 \xi_1 + \lambda_2 \xi_3 - \lambda_2 \xi_2 =
    \xi_1 \left( - \lambda_1 - \lambda_2 \right) + \lambda_1 \xi_2 + \lambda_2 \xi_3.$$

  Видим, что это линейная комбинация координат гауссовского вектора, значит,
  вектор действительно гауссовский.
  Чтобы указать распределение, нужны параметры,
  то есть вектор математических ожиданий и матрица ковариаций.

  $$ \vec{ \eta } =
    B \vec{ \xi } =
    \begin{bmatrix}
      \xi_2 - \xi_1 \\
      \xi_3 - \xi_2
    \end{bmatrix} =
    \begin{bmatrix}
      -1 & 1 & 0 \\
      0 & -1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \xi_1 \\
      \xi_2 \\
      \xi_3
    \end{bmatrix},$$
  где
  $$ \begin{bmatrix}
      -1 & 1 & 0 \\
      0 & -1 & 1
    \end{bmatrix} =
    B.$$

  Матрицей $B$ действуем на вектор математических ожидаий и на ковариационную матрицу
  $ \vec{m}_{ \eta } = B \vec{m}_{ \xi } = 0, \, A_{ \eta } = BA_{ \xi } B^T$, где $A_{ \xi }$ ---
  еденичная матрица, потому что все они независимы, некоррелируемы, значит,
  $$BA_{ \xi }B^T =
    BB^T =
    \begin{bmatrix}
      -1 & 1 & 0 \\
      0 & -1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 & 0 \\
      1 & -1 \\
      0 & 1
    \end{bmatrix}.$$
  Должна быть симметрическая матрица
  $$ \begin{bmatrix}
      -1 & 1 & 0 \\
      0 & -1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      -1 & 0 \\
      1 & -1 \\
      0 & 1
    \end{bmatrix} =
    \begin{bmatrix}
      2 & -1 \\
      -1 & 2
    \end{bmatrix}$$
  --- матрица ковариаций;
  \item скобками обозначена упорядоченность
  $ \vec{ \eta } =
    \left(
      \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \,
      \xi_{ \left( 3 \right) } - \xi_{ \left( 2 \right) }
    \right) $.

  Если бы знали плотность распределения
  $ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) $
  (знаем из задачи 1.10)
  $p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
    \left( x_1, x_2, x_3 \right) $,
  могли бы сказать,
  что вектор $ \vec{ \eta }$ получен линейным преобразованием с помощью матрицы $B$.

  Формула замены переменных
  $$p_{B \vec{ \xi }} \left( \vec{x} \right) =
    \frac{1}{det \, B} \cdot p_{ \vec{ \xi }} \left( B^{-1} \vec{x} \right).$$

  Значит, матрица $B$ должна быть только квадратной.
  Найдём плотность вектора
  $ \left(
      \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \,
      \xi_{ \left( 3 \right) } - \xi_{ \left( 2 \right) }
    \right) \,
    p \left( u_1, u_2, u_3 \right) $.

  Должны проинтегрировать по $u_3$.
  Это делают, когда размерности не совпадают.

  $p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
    \left( x_1, x_2, x_3 \right) =
    3! p_{ \left( \xi_1, \xi_2, \xi_3 \right) } \left( x_1, x_2, x_3 \right) \cdot
    \mathbbm{1} \left\{ x_1 < x_2 < x_3 \right\} $.
  Запишем для нашего случая.
  $ \xi_1, \xi_2, \xi_3$ --- независимые, значит, плотность вектора --- это произведение плотностей
  \begin{equation*}
    \begin{split}
      3! p_{ \left( \xi_1, \xi_2, \xi_3 \right) } \left( x_1, x_2, x_3 \right) \cdot
      \mathbbm{1} \left\{ x_1 < x_2 < x_3 \right\} = \\
      = 6p_{ \xi_1} \left( x_1 \right) p_{ \xi_2} \left( x_2 \right) p_{ \xi_3} \left( x_3 \right) \cdot
      \mathbbm{1} \left\{ x_1 < x_2 < x_3 \right\} = \\
      = \frac{6}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left( x_1^2 + x_2^2 + x_3^2 \right) } \cdot
      \mathbbm{1} \left\{ x_1 < x_2 < x_3 \right\}.
    \end{split}
  \end{equation*}

  Записываем преобразование
  $$ \begin{bmatrix}
      \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 3 \right) } - \xi_{ \left( 2 \right) } \\
      \xi_{ \left( 1 \right) }
    \end{bmatrix} =
    \begin{bmatrix}
      -1 & 1 & 0 \\
      0 & -1 & 1 \\
      1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 2 \right) } \\
      \xi_{ \left( 3 \right) }
    \end{bmatrix}.$$

  Значит, матрица
  $$B =
    \begin{bmatrix}
      -1 & 1 & 0 \\
      0 & -1 & 1 \\
      1 & 0 & 0
    \end{bmatrix}.$$

  Чтобы применить формулу замены переменных, нужны определитель и обратная матрица.
  Раскладываем по последней строке
  $$det \, B = 1, \,
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      \xi_{ \left( 2 \right) } - \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 3 \right) } - \xi_{ \left( 2 \right) } \\
      \xi_{ \left( 1 \right) }
    \end{bmatrix} =
    \begin{bmatrix}
      \xi_{ \left( 1 \right) } \\
      \xi_{ \left( 2 \right) } \\
      \xi_{ \left( 3 \right) }
    \end{bmatrix}.$$

  Значит,
  $$B^{-1} =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 1
    \end{bmatrix}.$$

  Действуем матрицей на вектор
  $$B^{-1} \vec{x} =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\
      x_2 \\
      x_3
    \end{bmatrix} =
    \begin{bmatrix}
      x_3 \\
      x_1 + x_3 \\
      x_1 + x_2 + x_3
    \end{bmatrix}.$$

  Значит,
  $$p_{B \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
      \left( \vec{x} \right) =
    \frac{1}{det \, B} \cdot
    p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
      \left( B^{-1} \vec{x} \right).$$
  Подставим значения плотностей
  \begin{equation*}
    \begin{split}
      \frac{1}{det \, B} \cdot
      p_{ \left( \xi_{ \left( 1 \right) }, \xi_{ \left( 2 \right) }, \xi_{ \left( 3 \right) } \right) }
        \left( B^{-1} \vec{x} \right) = \\
      = 6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left[ x_3^2 + \left( x_1 + x_3 \right)^2 + \left( x_1 + x_2 + x_3 \right)^2 \right] } \times \\
      \times \mathbbm{1} \left\{ x_3 < x_1 + x_3 < x_1 + x_2 + x_3 \right\}.
    \end{split}
  \end{equation*}
  Можем везде отнять $x_3$ в индикаторе
  \begin{equation*}
    \begin{split}
      6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left[ x_3^2 + \left( x_1 + x_3 \right)^2 + \left( x_1 + x_2 + x_3 \right)^2 \right] } \times \\
      \times \mathbbm{1} \left\{ x_3 < x_1 + x_3 < x_1 + x_2 + x_3 \right\} = \\
      = 6 \cdot \frac{1}{ \left( \sqrt{2 \pi } \right)^3} \cdot
      e^{- \frac{1}{2} \left[ x_3^2 + \left( x_1 + x_3 \right)^2 + \left( x_1 + x_2 + x_3 \right)^2 \right] } \cdot
      \mathbbm{1} \left\{ 0 < x_1 < x_1 + x_2 \right\}.
    \end{split}
  \end{equation*}
  Нужно проинтегрировать по $x_3$, то есть для начала выделить полный квадрат по $x_3$.

  Получим
  \begin{equation*}
    \begin{split}
      x_3^2 + x_1^2 + 2x_1 x_3 + x_3^2 + x_1^2 + x_2^2 + x_3^2 + 2x_1 x_2 + 2x_2 x_3 + 2x_1 x_3 = \\
      = 3x_3^2 + 2x_1^2 + x_2^2 + 4x_1 x_3 + 2x_1 x_2 + 2x_2 x_3 = \\
      = 3x_3^2 + 2x_3 \left( 2x_1 + x_2 \right) + 2x_1^2 + x_2^2 +2x_1x_2 = \\
      = \left( \sqrt{3} x_3 + \frac{2x_1 + x_2}{ \sqrt{3}} \right)^2 -
      \frac{ \left( 2x_1 + x_2 \right)^2}{3} + 2x_1^2 + x_2^2 + 2x_1 x_2 = \\
      = \left( \sqrt{3} x_3 + \frac{2x_1 + x_2}{ \sqrt{3}} \right)^2 -
      \frac{2}{3} \left( x_1^2 + x_2^2 + x_1 x_2 \right) .
    \end{split}
  \end{equation*}

  Нужная плотность вектора
  \begin{equation*}
    \begin{split}
      p_{ \left( \xi{ \left( 2 \right) } - \xi_{ \left( 1 \right) }, \xi_{ \left( 3 \right) } - \xi_{ \left( 2 \right) } \right) }
        \left( x_1, x_2 \right) = \\
      = \frac{6}{2 \pi } \cdot e^{- \frac{1}{2} \cdot
      \frac{2}{3} \left( x_1^2 + x_2^2 + x_1 x_2 \right) } \cdot
      \mathbbm{1} \left\{ 0 < x_1 < x_1 + x_2 \right\} \times \\
      \times \int \limits_{ \mathbb{R}}
        \frac{1}{ \sqrt{2 \pi }} \cdot
        e^{- \frac{1}{2} \left( \sqrt{3} x_3 + \frac{2x_1 + x_2}{ \sqrt{3}} \right)^2}
      dx_3 = \\
      = \frac{6}{2 \pi \sqrt{3}} \cdot e^{- \frac{1}{3} \left( x_1^2 + x_2^2 + x_1 x_2 \right) } =
      \frac{ \sqrt{3}}{ \pi } \cdot e^{- \frac{1}{3} \left(x_1^2 + x_2^2 + x_1 x_2 \right) }.
    \end{split}
  \end{equation*}
\end{enumerate}

\subsubsection*{7.14}

\textit{Задание.} Пусть $Z = XY$, где $X$ и $Y$ независимы, $X \sim N \left( 0, 1 \right) $,
а
$$P \left( Y = 1 \right) =
  P \left( Y = -1 \right) =
  \frac{1}{2}.$$
Докажите, что $Z \sim N \left( 0, 1 \right) $.
Найдите распределение пар $ \left( X, Z \right), \, \left( Y, Z \right) $
и распределение случайной величины $X + Z$.
Убедитесь в том, что $X$ и $Z$ являются некоррелируемыми, но зависимыми случайными величинами.

\textit{Решение.}
$$ \varphi_Z \left( t \right) =
  Me^{itZ} =
  Me^{itXY} =
  Me^{-itX} \cdot P \left( Y = -1 \right) + Me^{itX} \cdot P \left( Y = 1 \right).$$
По определению характеристической функции
$$Me^{-itX} \cdot P \left( Y = -1 \right) + Me^{itX} \cdot P \left( Y = 1 \right) =
  \varphi_X \left( -t \right) \cdot \frac{1}{2} + \varphi_X \left( t \right) \cdot \frac{1}{2}.$$
Подставляем значения характеристической функции
$$ \varphi_X \left( -t \right) \cdot \frac{1}{2} + \varphi_X \left( t \right) \cdot \frac{1}{2} =
  e^{- \frac{t^2}{2}} \cdot \frac{1}{2} + e^{- \frac{t^2}{2}} \cdot \frac{1}{2} =
  e^{- \frac{t^2}{2}},$$
значит, $Z \sim N \left( 0, 1 \right) $.

Найдём
$$ \varphi_{ \left( X, Z \right) } \left( t_1, t_2 \right) =
  Me^{it_1 X + it_2 Z} =
  Me^{it_1 X + it_2 XY}.$$
Подставляем возможные значения $Y$ и умножаем полученные выражения на соответствующие вероятности
$$Me^{it_1 X + it_2 XY} =
  Me^{it_1X + it_2 X} \cdot P \left( Y = 1 \right) +
  Me^{it_1 X - it_2 X} \cdot P \left( Y = -1 \right).$$
Группируем степени экспонент и подставляем значения вероятностей
\begin{equation*}
  \begin{split}
    Me^{it_1X + it_2 X} \cdot P \left( Y = 1 \right) +
    Me^{it_1 X - it_2 X} \cdot P \left( Y = -1 \right) = \\
    = Me^{iX \left( t_1 + t_2 \right) } \cdot \frac{1}{2} +
    Me^{iX \left( t_1 - t_2 \right) } \cdot \frac{1}{2} = \\
    = \varphi_X \left( t_1 + t_2 \right) \cdot \frac{1}{2} +
    \varphi_X \left( t_1 - t_2 \right) \cdot \frac{1}{2} =
    e^{- \frac{ \left( t_1 + t_2 \right)^2}{2}} \cdot \frac{1}{2} +
    e^{- \frac{ \left( t_1 - t_2 \right)^2}{2}} \cdot \frac{1}{2} = \\
    = \frac{1}{2} \cdot e^{- \frac{t_1^2 + 2t_1 t_2 + t_2^2}{2}} +
    \frac{1}{2} \cdot e^{- \frac{t_1^2 - 2t_1 t_2 + t_2^2}{2}} =
    \frac{1}{2} \cdot e^{- \frac{t_1^2 + t_2^2}{2}} \left( e^{-t_1 t_2} + e^{t_1 t_2} \right).
  \end{split}
\end{equation*}

Подобным образом находим
$$ \varphi_{ \left( Y, Z \right) } \left( t_1, t_2 \right) =
  Me^{it_1 Y + it_2 Z} =
  Me^{it_1 Y + it_2 XY}.$$
Подставляем возможные значения $Y$ и умножаем полученные выражения на соответствующие вероятности
$$Me^{it_1 Y + it_2 XY} =
  Me^{-it_1 -it_2 X} \cdot P \left( Y = -1 \right) +
  Me^{it_1 + it_2 X} \cdot P \left( Y = 1 \right).$$
Разбиваем экспоненты на две
\begin{equation*}
  \begin{split}
    Me^{-it_1 -it_2 X} \cdot P \left( Y = -1 \right) +
    Me^{it_1 + it_2 X} \cdot P \left( Y = 1 \right) = \\
    = M \left( e^{-it_1} e^{-it_2 X} \right) \cdot \frac{1}{2} +
    M \left( e^{it_1} e^{it_2 X} \right) \cdot \frac{1}{2} = \\
    = e^{-it_1} \cdot \varphi_X \left( -t_2 \right) \cdot \frac{1}{2} +
    e^{it_1} \varphi_X \left( t_2 \right) \cdot \frac{1}{2} =
    e^{-it_1} \cdot e^{- \frac{t_2^2}{2}} \cdot \frac{1}{2} +
    e^{it_1} \cdot e^{- \frac{t_2^2}{2}} \cdot \frac{1}{2} = \\
    = \frac{1}{2} \cdot e^{- \frac{t_2^2}{2}} \left( e^{-it_1} + e^{it_1} \right).
  \end{split}
\end{equation*}

По такому же принципу находим
$$ \varphi_{X + Z} \left( t \right) =
  Me^{it \left( X + Z \right) } =
  Me^{itX + itZ} =
  Me^{itX + itXY}.$$
Подставляем возможные значения $Y$ и умножаем полученные выражения на соответствующие вероятности
$$Me^{itX + itXY} =
  Me^{itX - itX} \cdot P \left( Y = -1 \right) + Me^{itX + itX} \cdot P \left( Y = 1 \right).$$
Приводим подобные в степенях экспонент и подставляем значения вероятностей
$$Me^{itX - itX} \cdot P \left( Y = -1 \right) + Me^{itX + itX} \cdot P \left( Y = 1 \right) =
  \frac{1}{2} + Me^{2itX} \cdot \frac{1}{2}.$$
По определению характеристической функции
$$ \frac{1}{2} + Me^{2itX} \cdot \frac{1}{2} =
  \frac{1}{2} + \varphi_X \left( 2t \right) \cdot \frac{1}{2} =
  \frac{1}{2} + e^{- \frac{4t^2}{2}} \cdot \frac{1}{2} =
  \frac{1}{2} + e^{-2t^2} \cdot \frac{1}{2} =
  \frac{1}{2} \left( 1 + e^{-2t^2} \right).$$

Характеристическая функция независимых случайных
величин равна произведению их характеристических функций.

$ \varphi_X \left( t \right) =
  Me^{itX} =
  e^{- \frac{t^2}{2}}$.

Найдём произведение характеристических функций случайных величин
$$ \varphi_X \left( t \right) \varphi_Z \left( t \right) =
  e^{- \frac{t^2}{2}} e^{- \frac{t^2}{2}} =
  e^{-t^2} \neq
  \frac{1}{2} \left( 1 + e^{-2t^2} \right) =
  \varphi_{X + Z} \left( t \right),$$
значит, случайные величины $X$ и $Z$ --- зависимы.

$M \left( XZ \right) =
  M \left( XXY \right) =
  M \left( X^2 Y \right) =
  MX^2 \cdot MY =
  -1 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} =
  0$,
значит, они некоррелируемые.


\subsubsection*{7.15}

\textit{Задание.}
Пусть $ \xi $ --- случайная величина со стандартным нормальным распределением, и пусть
$$ \eta_{ \alpha } =
  \begin{cases}
    \xi, \qquad if \left| \xi \right| \leq \alpha, \\
    - \xi, \qquad if \left| \xi \right| \geq \alpha.
  \end{cases}$$
Докажите, что $ \eta_{ \alpha } \sim N \left( 0, 1 \right) $ и что при $ \alpha $ таком, что
$$ \int \limits_0^{ \alpha } x^2 f_{ \xi } \left( x \right) dx =
  \frac{1}{4}$$
величины $ \xi $ и $ \eta_{ \alpha }$ являются некоррелируемыми,
но зависимыми гауссовскими случайными величинами.

\textit{Решение.} $ \xi \sim N \left( 0, 1 \right) $.

Запишем $ \eta_{ \alpha }$ через индикаторы
$ \eta_{ \alpha } =
  \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} -
  \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\} $.

Характеристическая функция
$$ \varphi_{ \eta_{ \alpha }} \left( t \right) =
  Me^{it \eta_{ \alpha }} =
  Me^{it \left( \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} - \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\} \right) }.$$
Раскрываем скобки
$$Me^{it \left( \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} - \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\} \right) } =
  Me^{it \xi } \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} +
  Me^{-it \xi } \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\}.$$
Подставляем значения характеристической функции стандартной нормальной случайной величины
$$Me^{it \xi } \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} +
  Me^{-it \xi } \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\} =
  e^{- \frac{t^2}{2}} \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} +
  e^{- \frac{t^2}{2}} \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\}.$$
Объединим индикаторы
$$e^{- \frac{t^2}{2}} \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} +
  e^{- \frac{t^2}{2}} \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\} =
  e^{- \frac{t^2}{2}},$$
следовательно, $ \eta_{ \alpha } \sim N \left( 0, 1 \right) $.

Покажем, что случайные величины $ \xi $ и $ \eta_{ \alpha }$ некоррелируемы при
$$ \int \limits_0^{ \alpha } x^2 f_{ \xi } \left( x \right) dx =
  \frac{1}{4}.$$
Нужно показать,
что
$Cov \left( \xi, \eta_{ \alpha } \right) =
  M \left( \xi \eta_{ \alpha } \right) - M \xi \cdot M \eta_{ \alpha } =
  M \left( \xi \eta_{ \alpha } \right) =
  0$.

Покажем это
$$M \left( \xi \eta_{ \alpha } \right) =
  M \left[
    \xi \left(
      \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} -
      \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\}
    \right)
  \right].$$
Раскроем скобки и воспользуемся линейностью математического ожидания
\begin{equation*}
  \begin{split}
    M \left[
    \xi \left(
        \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} -
        \xi \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\}
      \right)
    \right] = \\
    = M \left( \xi^2 \cdot \mathbbm{1} \left\{ \left| \xi \right| \leq \alpha \right\} \right) -
    M \left( \xi^2 \cdot \mathbbm{1} \left\{ \left| \xi \right| \geq \alpha \right\} \right) = \\
    = \int \limits_{- \alpha }^{ \alpha } x^2 f_{ \xi } \left( x \right) dx -
    \int \limits_{- \infty }^{ \alpha } x^2 f_{ \xi } \left( x \right) dx -
    \int \limits_{ \alpha }^{+ \infty } x^2 f_{ \xi } \left( x \right) dx = \\
    = \int \limits_{- \alpha }^{ \alpha } x^2 f_{ \xi } \left( x \right) dx - \left(
      \int \limits_{- \infty }^{+ \infty } x^2 f_{ \xi } \left( x \right) dx -
      \int \limits_{- \alpha }^{ \alpha } x^2 f_{ \xi } \left( x \right) dx
    \right) = \\
    = -1 + 2 \int \limits_{- \alpha }^{ \alpha } x^2 f_{ \xi } \left( x \right) dx =
    -1 + 4 \int \limits_0^{ \alpha } x^2 f_{ \xi } \left( x \right) dx =
    -1 + 4 \cdot \frac{1}{4} =
    -1 + 1 =
    0.
  \end{split}
\end{equation*}

Значит, $Cov \left( \xi, \eta_{ \alpha } \right) = 0$, то есть они некоррелируемые.

$ \xi $ и $ \eta_{ \alpha }$ зависимы, так как $ \eta_{ \alpha }$ выражается через $ \xi $.
