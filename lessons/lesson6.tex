\addcontentsline{toc}{chapter}{Занятие 6. Теорема Колмогорова про улучшение оценок.
                              Неравенство Рао-Крамера}
\chapter*{Занятие 6. Теорема Колмогорова про улучшение оценок. Неравенство Рао-Крамера}

\addcontentsline{toc}{section}{Контрольные вопросы и задания}
\section*{Контрольные вопросы и задания}

\subsubsection*{Приведите определение достаточной статистики.}

Статистика $T$ называется достаточной для параметра $ \theta $,
если условное распределение при известном $T$ не зависит от параметра $ \theta $.

\subsubsection*{Сформулируйте теорему про характеризацию достаточной статистики.}

Пусть $x_1, \dotsc, x_n$ ---
выборка из распределения с плотностью
$$p \left( x, \theta \right), \,
  \theta \in \Theta.$$

Статистика $T$ является достаточной тогда и только тогда,
когда функция правдоподобия $L \left( \vec{x}, \theta \right) $ допускает факторизацию,
то есть может быть представлена произведением двух функций следующего вида
$$L \left( \vec{x}, \theta \right) =
  h \left( T, \theta \right) \cdot g \left( \vec{x} \right).$$

\subsubsection*{Сформулируйте теорему Колмогорова про улучшение
                оценки с помощью достаточной статистики.}

Оптимальная оценка единственная (в том случае, когда она существует).

\subsubsection*{Что называется количеством информации Фишера?}

$$D_{ \theta } U \left( \vec{x}, \theta \right) =
  nD_{ \theta } \left(
    \frac{ \partial }{ \partial \theta } ln \, p \left( x, \theta \right)
  \right) =
  nI_0,$$
где $nI_0$ --- количество информации Фишера.

\subsubsection*{Запишите неравенство Рао-Крамера.}

Пусть $ \hat{ \theta }$ --- несмещённая оценка для параметра $ \theta $.
Тогда
$$ \forall \theta \in \Theta: \,
  D_{ \theta } \hat{ \theta } \geq \frac{1}{nI_0}.$$
Как угодно лучшую оценку взять нельзя.

\subsubsection*{Какая оценка называется эффективной?}

Если $ \hat{ \theta }$ такова, что $ \hat{ \theta }$ --- несмещённая и
$$D_{ \theta } \hat{ \theta } =
  \frac{1}{nI_0}, \,
  \theta \in \Theta,$$
то $ \hat{ \theta }$ называется эффективной оценкой.

\subsubsection*{Приведите определение экспоненциальной семьи распределений.}

\addcontentsline{toc}{section}{Аудиторные задачи}
\section*{Аудиторные задачи}

\subsubsection*{6.3}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ ---
выборка из равномерного распределения на отрезке $ \left[ 0, \theta \right].$
Найдите несмещённую оценку неизвестного параметра
$ \tau \left( \theta, y \right) =
  P \left( X_1 > y \right) $
и улучшите её усреднением по достаточной для параметра $ \theta $ статистике.

\textit{Решение.} $X_{ \left( n \right) }$ --- достаточная статистика для $ \theta $.

Нужно найти $ \hat{ \tau }$ из условия, что она была бы несмещённой,
что
$$M \hat{ \tau } =
  \tau =
  P \left( X_1 > y \right).$$
Берём в качестве $ \hat{ \tau } = \mathbbm{1} \left\{ X_1 > y \right\} $ ---
это несмещённая оценка для $ \tau $.
Теперь улучшаем эту оценку.
$ \tau^* =
  M \left( \hat{ \tau } \; \middle| \; X_{ \left( n \right) } \right) =
  M \left\{ \mathbbm{1} \left\{ X_1 < y \right\} \; \middle| \; X_{ \left( n \right) } \right\}.$
Индикатор может принимать значения 0 и 1,
значит
\begin{equation*}
  \begin{split}
    M \left\{
      \mathbbm{1} \left\{ X_1 < y \right\} \; \middle| \; X_{ \left( n \right) }
    \right\} = \\
    = 0 \cdot
    P \left(
      \mathbbm{1} \left\{ X_1 > y \right\} = 0 \; \middle| \; X_{ \left( n \right) }
    \right) +
    1 \cdot
    P \left( \mathbbm{1} \left\{ X_1 > y \right\} = 1 \; \middle| \; X_{ \left( n \right) } \right).
  \end{split}
\end{equation*}
Первое слагаемое пропадает
\begin{equation*}
  \begin{split}
    0 \cdot
    P \left(
      \mathbbm{1} \left\{ X_1 > y \right\} = 0 \; \middle| \; X_{ \left( n \right) }
    \right) +
    1 \cdot
    P \left(
      \mathbbm{1} \left\{ X_1 > y \right\} = 1 \; \middle| \; X_{ \left( n \right) }
    \right) = \\
    = P \left( X_1 > y \; \middle| X_{ \left( n \right) } \right) =
    f \left( X_{ \left( n \right) } \right).
  \end{split}
\end{equation*}
Ищем $f \left( z \right) = P \left( X_1 > y \; \middle| \; X_{ \left( n \right) } = z \right).$
Выборка имеет равномерное распределение, то есть распределение, которое имеет плотность.
Вероятность попадания в точку равна нулю
$$ P \left( X_1 > y \; \middle| \; X_{ \left( n \right) } = z \right) =
  \lim \limits_{h \to 0}
    P \left\{ X_1 > y \; \middle| \; X_{ \left( n \right) } \in \left[ z, z + h \right] \right\}.$$
Воспользуемся определением условной вероятности
$$ \lim \limits_{h \to 0}
    P \left\{ X_1 > y \; \middle| \; X_{ \left( n \right) } \in \left[ z, z + h \right] \right\} =
  \lim \limits_{h \to 0}
    \frac{P \left\{ X_1 > y, X_{ \left( n \right) } \in \left[ z, z + h \right] \right\} }{P \left\{ X_{ \left( n \right) } \in \left[ z, z + h \right] \right\} }.$$
Это нужно рассматривать при $y < z$.
Разбиваем на разность двух вероятностей в числителе и знаменателе
\begin{equation*}
  \begin{split}
    \lim \limits_{h \to 0}
      \frac{P \left\{ X_1 > y, X_{ \left( n \right) } \in \left[ z, z + h \right] \right\} }{P \left\{ X_{ \left( n \right) } \in \left[ z, z + h \right] \right\} } = \\
    = \lim \limits_{h \to 0} \left[
      \frac{P \left\{ X_1 > y, X_{ \left( n \right) } \leq z + h \right\} }{P \left\{ X_{ \left( n \right) } \leq z + h \right\} - P \left\{ X_{ \left( n \right) } \leq z \right\} } -
      \frac{P \left\{ X_1 > y, X_{ \left( n \right) } \leq z \right\} }{P \left\{ X_{ \left( n \right) } \leq z + h \right\} - P \left\{ X_{ \left( n \right) } \leq z \right\} }
    \right].
  \end{split}
\end{equation*}
Переходим к функции распределения выборки
\begin{equation*}
  \begin{split}
    \lim \limits_{h \to 0} \left[
      \frac{P \left\{ X_1 > y, X_{ \left( n \right) } \leq z + h \right\} }{P \left\{ X_{ \left( n \right) } \leq z + h \right\} - P \left\{ X_{ \left( n \right) } \leq z \right\} } -
      \frac{P \left\{ X_1 > y, X_{ \left( n \right) } \leq z \right\} }{P \left\{ X_{ \left( n \right) } \leq z + h \right\} - P \left\{ X_{ \left( n \right) } \leq z \right\} }
    \right] = \\
    = \lim \limits_{h \to 0} \left\{
      \frac{ \left[ F \left( z + h \right) - F \left( y \right) \right] \left[ F \left( z + h \right) \right]^{n - 1}}{ \left[ F \left( z + h \right) \right]^n - \left[ F \left( z \right) \right]^n} -
      \frac{ \left[ F \left( z \right) - F \left( y \right) \right] \left[ F \left( z \right) \right]^{n - 1}}{ \left[ F \left( z + h \right) \right]^n - \left[ F \left( z \right) \right]^n}
    \right\}.
  \end{split}
\end{equation*}

Знаменатель и числитель стремятся к нулю.
Дифференцируем по $h$ числитель и знаменатель.
Это правило Лопиталя
\begin{equation*}
  \begin{split}
    \lim \limits_{h \to 0} \left\{
      \frac{ \left[ F \left( z + h \right) - F \left( y \right) \right] \left[ F \left( z + h \right) \right]^{n - 1}}{ \left[ F \left( z + h \right) \right]^n - \left[ F \left( z \right) \right]^n} -
      \frac{ \left[ F \left( z \right) - F \left( y \right) \right] \left[ F \left( z \right) \right]^{n - 1}}{ \left[ F \left( z + h \right) \right]^n - \left[ F \left( z \right) \right]^n}
    \right\} = \\
    = \lim \limits_{h \to 0} \left\{
      \frac{p \left( z + h \right) \left[ F \left( z + h \right) \right]^{n - 1}}{n \left[ F \left( z + h \right) \right]^{n - 1} p \left( z + h \right) } +
      \frac{ \left( n + 1 \right) \left[ F \left( z + h \right) \right]^{n - 2} p \left( z + h \right) \left[ F \left( z + h \right) - F \left( y \right) \right] }{n \left[ F \left( z + h \right) \right]^{n - 1} p \left( z + h \right) }
    \right\} = \\
    = \lim \limits_{h \to 0}
      \frac{F \left( z + h \right) + \left( n - 1 \right) \left[ F \left( z + h \right) - F \left( y \right) \right] }{nF \left( z + h \right) }.
  \end{split}
\end{equation*}
Переходим к пределу
$$ \lim \limits_{h \to 0}
    \frac{F \left( z + h \right) + \left( n - 1 \right) \left[ F \left( z + h \right) - F \left( y \right) \right] }{nF \left( z + h \right) } =
  \frac{F \left( z \right) + \left( n + 1 \right) \left[ F \left( z \right) - F \left( y \right) \right] }{nF \left( z \right) }.$$
Приводим подобные
$$ \frac{F \left( z \right) + \left( n + 1 \right) \left[ F \left( z \right) - F \left( y \right) \right] }{nF \left( z \right) } =
  \frac{nF \left( z \right) - \left( n - 1 \right) F \left( y \right) }{nF \left( z \right) }.$$
Почленно поделим числитель на знаменатель
$$ \frac{nF \left( z \right) - \left( n - 1 \right) F \left( y \right) }{nF \left( z \right) } =
  1 - \frac{n - 1}{n} \cdot \frac{F \left( y \right) }{F \left( z \right) }.$$
Подставляем значения функции равномерного распределения на отрезке $ \left[ 0, \theta \right] $.
Получаем
$$1 - \frac{n - 1}{n} \cdot \frac{F \left( y \right) }{F \left( z \right) } =
  1 - \frac{n - 1}{n} \cdot \frac{y}{z}.$$

Окончательно
$$f \left( X_{ \left( n \right) } \right) =
  1 - \frac{n - 1}{n} \cdot \frac{y}{X_{ \left( n \right) }}.$$

\subsubsection*{6.4}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ --- выборка из распределения Пуассона с параметром $ \lambda $.
В качестве оценки параметра $ \theta = e^{- \lambda }$ рассматривают статистику
$ \hat{ \theta } =
  \mathbbm{1} \left\{ X_1 = 0 \right\} $.
Вычислите смещение $b_n \left( \theta \right) = M \hat{ \theta } - \theta $
этой оценки и улучшите её усреднением по достаточной статистике для параметра
$ \theta $ статистикой.

\textit{Решение.} Неизвестный параметр --- это $e^{- \lambda } = \theta $.

Ищем смещение
$M \hat{ \theta } =
  M \mathbbm{1} \left\{ X_1 = 0 \right\} =
  P \left\{ X_1 = 0 \right\} =
  e^{- \lambda } =
  \theta $,
значит, смещение $b_n \left( \theta \right) = 0$.
Это несмещённая оценка.

Займёмся поиском достаточной статистики
$$L \left( \vec{X}, \lambda \right) =
  \prod \limits_{i = 1}^n \frac{ \lambda^{X_i}}{X_i!} \cdot e^{- \lambda } =
  \frac{ \lambda^{n \overline{X}}}{X_1! \dotsc X_n!} \cdot e^{- \lambda n} =
  \frac{1}{X_1! \dotsc X_n!} \cdot \lambda^{n \overline{X}} e^{- \lambda n}.$$

Вывод: $ \overline{X}$ --- достаточная статистика для $ \lambda $, а значит и для $ \theta $,
как функции от $ \lambda $.

Нудно найти условное математическое ожидание
$M \left( \hat{ \theta } \; \middle| \; \overline{X} \right) =
  f \left( \overline{X} \right) $.
Начнём с
$f \left( y \right) =
  M \left( \hat{ \theta } \; \middle| \; \overline{X} = y \right) =
  M \left( \mathbbm{1} \left\{ X_1 = 0 \right\} \; \middle| \; \overline{X} = y \right) $.
Индикатор --- это дискретная случайная величина, которая принимает 2 значения
$$M \left( \mathbbm{1} \left\{ X_1 = 0 \right\} \; \middle| \; \overline{X} = y \right) =
  1 \cdot P \left( X_1 = 0 \; \middle| \; \overline{X} = y \right) =
  \frac{P \left( X_1 = 0, \sum \limits_{i = 1}^n X_i = ny \right) }{P \left( \sum \limits_{i = 1}^n X_i = ny \right) }.$$
В числителе стоит случайное событие $X_1 = 0$.
Оно означает, что суммировать можно от двух
$$ \frac{P \left( X_1 = 0, \sum \limits_{i = 1}^n X_i = ny \right) }{P \left( \sum \limits_{i = 1}^n X_i = ny \right) } =
  \frac{P \left( X_1 = 0 \right) P \left( \sum \limits_{i = 2}^n X_i = ny \right) }{P \left( \sum \limits_{i = 1}^n X_i = ny \right) }.$$
Сумма независимых пуассоновских случайных величин имеет распределение Пуассона с параметром,
который равен сумме параметров
$$ \frac{P \left( X_1 = 0 \right) P \left( \sum \limits_{i = 2}^n X_i = ny \right) }{P \left( \sum \limits_{i = 1}^n X_i = ny \right) } =
  \frac{e^{- \lambda } \cdot \frac{ \left( \left( n - 1 \right) \lambda \right)^{ny}}{ \left( ny \right)!} \cdot e^{- \lambda \left( n - 1 \right) }}{ \frac{ \left( n \lambda \right)^{ny}}{ \left( ny \right)!} \cdot e^{- \lambda n}} =
  \left( \frac{n - 1}{n} \right)^{ny}.$$

Чтобы записать ответ, нужно вместо $y$ записать $ \overline{X}$.
Получим
$$ \left( \frac{n - 1}{n} \right)^{n \overline{X}}.$$
Когда $n \to \infty $, эта оценка стремится к $e^{- \overline{X}}$.

\subsubsection*{6.5}

\textit{Задание.}
Пользуясь неравенством Рао-Крамера, выясните,
является ли эффективной оценка $ \overline{X}$ параметра $p$ распределения Бернулли.

\textit{Решение.} $X_1, \dotsc, X_n$ --- выборка.

Несмещённой оценкой для параметра является $ \hat{p} = \overline{X}$.
Формально в этом убедимся.
Итак,
$$M \hat{p} =
  M \overline{X} =
  \frac{1}{n} \cdot M \sum \limits_{i = 1}^n X_i.$$
Все случайные величины одинаково распределены, следовательно, все математические ожидания совпадают
$$ \frac{1}{n} \cdot M \sum \limits_{i = 1}^n X_i =
  \frac{1}{n} \cdot n MX_1 =
  MX_1 =
  p.$$

Ищем дисперсию
$$D \hat{p} =
  D \overline{X} =
  D \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n X_i.$$
Дисперсии равны.
Случайные величины независимы, следовательно, дисперсия суммы --- это сумма дисперсий
$$ \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n X_i =
  \frac{1}{n^2} \cdot nDX_1 =
  \frac{1}{n} \cdot p \left( 1 - p \right).$$

Сначала нужна функция правдоподобия для одного наблюдения
$$L \left( X_1, p \right) =
  p^{X_1} \left( 1 - p \right)^{1 - X_1}.$$
Логарифмируем
$ln \, L \left( X_1, p \right) =
  X_1 \, ln \, p + \left( 1 - p \right) ln \left( 1 - p \right) $.
Ищем первую производную
$$ \frac{ \partial }{ \partial p} ln \, L \left( X_1, p \right) =
  \frac{X_1}{p} - \frac{1 - X_1}{1 - p}.$$
Ищем вторую производную
$$ \frac{ \partial^2}{ \partial p^2} ln \, L \left( X_1, p \right) =
  - \frac{X_1}{p^2} - \frac{1 - X_1}{ \left( 1 -p \right)^2}.$$
Пишем количество информации.
Берём математическое ожидание со знаком <<минус>>
$$I \left( \theta \right) =
  M \left( \frac{X_1}{p} + \frac{1 - X_1}{ \left( 1 - p \right)^2} \right) =
  \frac{1}{p} + \frac{1 - p}{ \left( 1 - p \right)^2} =
  \frac{1}{p} + \frac{1}{1 - p}.$$
Приводим к общему знаменателю
$$ \frac{1}{p} + \frac{1}{1 - p} =
  \frac{1 - p + p}{p \left( 1 - p \right) } =
  \frac{1}{p \left( 1 - p \right) }.$$

Ищем обратную величину, делённую на $n$.
Получаем
$$ \frac{1}{nI \left( \theta \right) } =
  \frac{1}{n \left[ p \left( 1 - p \right) \right]^{-1}} =
  \frac{p \left( 1 - p \right) }{n} =
  D \hat{p},$$
следовательно, $ \hat{p}$ --- эффективная оценка параметра $p$.

\subsubsection*{6.6}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ --- выборка из равномерного рпспределения с параметром $p$.
Выясните, является ли эффективной оценка $ \hat{ \theta } = 1 + \overline{X}$ параметра
$$ \theta =
  \frac{1}{p}.$$

\textit{Решение.} Берём дисперсию
$$D \hat{ \theta } =
  D \left( 1 + \overline{X} \right) =
  D \overline{X} =
  D \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n X_i =
  \frac{1}{n^2} \cdot nDX_1.$$
Сократим $n$ и подставим значение дисперсии равномерного распределения
$$ \frac{1}{n^2} \cdot nDX_1 =
  \frac{1}{n} \cdot \frac{1 - p}{p^2}.$$
Можем записать это как функцию от $ \theta $.
Получим
$$ \frac{1}{n} \cdot \frac{1 - p}{p^2} =
  \frac{1}{n} \cdot \left( \theta^2 - \theta \right).$$
Можно $ \theta $ вынести за скобки
$$ \frac{1}{n} \cdot \left( \theta^2 - \theta \right) =
  \frac{ \theta }{n} \cdot \left( \theta - 1 \right).$$
Записываем функцию правдоподобия для одного наблюдения
$$L \left( X_1, p \right) =
  p \left( 1 - p \right)^{X_1}.$$

Подставим оценку параметра
$$L \left( X_1, \theta \right) =
  \frac{1}{ \theta } \cdot \left( 1 - \frac{1}{ \theta } \right)^{X_1} =
  \frac{1}{ \theta^{X_1 + 1}} \cdot \left( \theta - 1 \right)^{X_1}.$$

Логарифмируем
$ln \, L \left( X_1, \theta \right) =
  - \left( X_1 + 1 \right) ln \, \theta + X_1 ln \left( \theta - 1 \right) $.
Берём первую производную
$$ \frac{ \partial }{ \partial \theta } ln \, L \left( X_1, \theta \right) =
  - \frac{X_1 + 1}{ \theta } + \frac{X_1}{ \theta - 1}.$$

Берём вторую производную
$$ \frac{ \partial^2}{ \partial \theta^2} ln \, L \left( X_1, \theta \right)  =
  \frac{X_1 + 1}{ \theta^2} - \frac{X_1}{ \left( \theta - 1 \right)^2}.$$
Берём математическое ожидание
$$MX_1 =
  \frac{1 - p}{p}.$$
Почленно поделим числитель на знаменатель
$$ \frac{1 - p}{p} =
  \theta - 1.$$

Берём математическое ожидание второй производной функции правдоподобия со знаком <<минус>>
$$-M \left[ \frac{ \partial^2}{ \partial \theta^2} ln \, L \left( X_1, \theta \right) \right] =
  - \frac{ \theta }{ \theta^2} + \frac{ \theta - 1}{ \left( \theta - 1 \right)^2} =
  - \frac{1}{ \theta } + \frac{1}{ \theta - 1} =
  \frac{- \theta + 1 + \theta }{ \theta \left( \theta - 1 \right) }.$$
Приводим подобные
$$ \frac{- \theta + 1 + \theta }{ \theta \left( \theta - 1 \right) } =
  \frac{1}{ \theta \left( \theta - 1 \right) } =
  I \left( \theta \right).$$

Формируем обратную величину и делим на $n$.
Значит,
$$ \frac{1}{nI \left( \theta \right) } =
  \frac{ \theta \left( \theta - 1 \right) }{n} =
  D \hat{ \theta }.$$
Получили, что оценка эффективная.

\subsubsection*{6.7}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ ---
выборка из распределения $N \left( \theta, \sigma^2 \right) $ с неизвестным параметром $ \theta $
и известной дисперсией.
Выясните,
является ли статистика $ \theta^* = \overline{X}$ эффективной оценкой параметра $ \theta $.

\textit{Решение.} Проверим несмещённость
$$M \theta^* =
  M \overline{X} =
  M \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n} \cdot M \sum \limits_{i = 1}^n X_1 =
  \frac{1}{n} \cdot nMX_1 =
  \theta.$$
Несмещённость есть.
Ищем дисперсию
$$D \theta^* =
  D \overline{X} =
  D \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n X_i.$$
Воспользуемся независимостью случайных величин в выборке
$$ \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n X_i =
  \frac{1}{n^2} \cdot nDX_1.$$
Случайные величины в выборке одинаково распределены, значит
$$ \frac{1}{n^2} \cdot nDX_1 =
  \frac{ \sigma^2}{n}.$$

Переходим к функции правдоподобия и оценке количества информации.
Смотрим на одно наблюдение
$$L \left( X_1, \theta \right) =
  \frac{1}{ \sqrt{2 \pi } \sigma } \cdot e^{- \frac{ \left( X_1 - \theta \right) }{2 \sigma^2}}.$$
Логарифмируем
$$ln \, L \left( X_1, \theta \right) =
  ln \left( \frac{1}{ \sqrt{2 \pi } \sigma } \right) -
  \left( X_1 - \theta \right)^2 \cdot \frac{1}{2 \sigma^2}.$$

Ищем первую производную
$$ \frac{ \partial }{ \partial \theta } ln \, L \left( X_1, \theta \right) =
  2 \left( X_1 - \theta \right) \cdot \frac{1}{2 \sigma^2} =
  \frac{X_1 - \theta }{ \sigma^2}.$$
Ищем вторую производную
$$ \frac{ \partial }{ \partial \theta^2} ln \, L \left( X_1, \theta \right) =
  - \frac{1}{ \sigma^2}.$$
Таким образом,
$$I \left( \theta \right) =
  -M \left( - \frac{1}{ \sigma^2} \right) =
  \frac{1}{ \sigma^2}.$$
Формируем величину
$$ \frac{1}{nI \left( \theta \right) } =
  \frac{ \sigma^2}{n} =
  D \theta^*,$$
значит, оценка эффективная.

\subsubsection*{6.8}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ --- выборка из экспоненциального распределения с плотностью
$$f_{ \theta } \left( x \right) =
  \begin{cases}
    \frac{1}{ \theta } \cdot e^{- \frac{x}{ \theta }}, \qquad x \geq 0, \\
    0, \qquad x < 0.
  \end{cases}$$
Убедитесь, что оценка $ \theta^* = \overline{X}$ ---
эффективная по Рао-Крамеру оценка неизвестного параметра $ \theta $.

\textit{Решение.}
$$M \theta^* =
  M \overline{X} =
  M \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n} \cdot M \sum \limits_{i = 1}^n X_i =
  \frac{1}{n} \cdot nMX_1 =
  MX_1 =
  \theta,$$
значит, имеем несмещённость.

Теперь ищем дисперсию
$$D \overline{X} =
  \frac{1}{n} \cdot DX_1 =
  \frac{1}{n} \cdot \frac{1}{ \left( \frac{1}{ \theta } \right)^2} =
  \frac{ \theta^2}{n}.$$

Ищем количество информации.
Начнём с функции правдоподобия
$$L \left( X_1, \theta \right) =
  \frac{1}{ \theta } \cdot e^{- \frac{X_1}{ \theta }} \cdot
  \mathbbm{1} \left\{ X_1 \geq 0 \right\}.$$
Пусть $X_1 \geq 0$.
Тогда берём логарифм
$$ln \, L \left( X_1, \theta \right) =
  - ln \, \theta - \frac{X_1}{ \theta }.$$

Берём первую производную от логарифма функции прадоподобия
$$ \frac{ \partial ln \, L}{ \partial \theta } =
  - \frac{1}{ \theta } + \frac{X_1}{ \theta^2}.$$

Берём вторую производную от логарифма функции правдоподобия
$$ \frac{ \partial ^2 ln \, L}{ \partial \theta^2} =
  \frac{1}{ \theta^2} - \frac{2X_1}{ \theta^3}.$$

Ищем количество информации
$$I \left( \theta \right) =
  -M \left( \frac{1}{ \theta^2} - \frac{2X_1}{ \theta^3} \right) =
  - \frac{1}{ \theta^2} + \frac{2}{ \theta^3} \cdot MX_1 =
  - \frac{1}{ \theta^2} + \frac{2}{ \theta^2} =
  \frac{1}{ \theta^2}.$$

Ищем обратную величину к количеству информации и делим на $n$.
Получаем
$$ \frac{1}{nI \left( \theta \right) } =
  \frac{ \theta^2}{n} =
  D \theta^*.$$
Значит, выборочное среднее --- эффективная оценка.

\addcontentsline{toc}{section}{Домашнее задание}
\section*{Домашнее задание}

\subsubsection*{6.10}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ ---
выборка из нормального распределения с параметрами $ \left( a, 1 \right) $.
Улучшите оценку $ \hat{a} = X_1$ усреднением по фиксированному значению достаточной
статистики $ \overline{X}$.
Найдите распределение, математическое ожидание и дисперсию улучшенной оценки.

\textit{Решение.} Улучшаем оценку
$$a^* =
  M \left( \hat{a} \; \middle| \; \overline{X} \right) =
  M \left( X_1 \; \middle| \; \overline{X} \right) =
  \frac{1}{n} \sum \limits_{i = 1}^n M \left( X_i \; \middle| \; \overline{X} \right) =
  M \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \; \middle| \; \overline{X} \right).$$
Запишем через выборочное среднее
$$M \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \; \middle| \; \overline{X} \right) =
  M \left( \overline{X} \; \middle| \; \overline{X} \right) =
  \overline{X}.$$

Математическое ожидание равно
$$Ma^* =
  M \overline{X} =
  M \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n} \sum \limits_{i = 1}^n MX_i =
  \frac{1}{n} \cdot na =
  a,$$
дисперсия ---
$$Da^* =
  D \left( \frac{1}{n} \sum \limits_{i = 1}^n X_i \right) =
  \frac{1}{n^2} \cdot n DX_1 =
  \frac{1}{n} \cdot 1 =
  \frac{1}{n}.$$

Она имеет нормальное распределение
$$a^* \sim
  N \left( a, \frac{1}{n} \right).$$

Так как $Ma^* = a$, то оценка несмещённая.

\subsubsection*{6.11}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ --- выборка из распределения Пуассона с параметром $ \lambda $.
В качестве оценки параметра $ \theta = e^{- \lambda }$ рассматривают статистику
$$ \hat{ \theta } =
  \overline{ \mathbbm{1} \left\{ X = 0 \right\} } =
  \frac{1}{n} \sum \limits_{i = 1}^n \mathbbm{1} \left\{ X_i = 0 \right\}.$$
Вычислите смещение $b_n \left( \theta \right) = M \hat{ \theta } - \theta $ этой оценки и выясните,
является ли она эффективной.

\textit{Решение.}
$$M \hat{ \theta } =
  M \left( \frac{1}{n} \sum \limits_{i = 1}^n \mathbbm{1} \left\{ X_i = 0 \right\} \right) =
  \frac{1}{n} M \left( \sum \limits_{i = 1}^n \mathbbm{1} \left\{ X_i = 0 \right\} \right) =
  \frac{1}{n} \cdot \sum \limits_{i = 1}^n P \left( X_i = 0 \right).$$
Подставляем значение вероятности для распределения Пуассона
$$ \frac{1}{n} \cdot \sum \limits_{i = 1}^n P \left( X_i = 0 \right) =
  \frac{1}{n} \cdot \sum \limits_{i = 1}^n \frac{ \lambda^0}{0!} \cdot e^{- \lambda } =
  \frac{1}{n} \cdot e^{- \lambda } \cdot n =
  e^{- \lambda }.$$

Подставляем полученное значение в формулу для смещения
$$b_n \left( \theta \right) =
  M \hat{ \theta } - \theta =
  e^{- \lambda } - e^{- \lambda } =
  0.$$

\subsubsection*{6.12}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ ---
выбока из распределения с плотностью
$f_{ \theta } \left( y \right) = \theta y^{ \theta - 1}, \,
  y \in \left( 0, 1 \right) $,
где $ \theta > 0$.
Докажите, что статистика $- \overline{ln \, X}$ несмещённая и эффективная оценка для параметра
$$ \tau =
  \frac{1}{ \theta }.$$

\textit{Решение.}
$$M \left( - \overline{ln \, X} \right) =
  M \left( - \frac{1}{n} \sum \limits_{i = 1}^n ln \, X_i \right) =
  - \frac{1}{n} \cdot M \sum \limits_{i = 1}^n ln \, X_i =
  - \frac{1}{n} \cdot nM \, ln \, X_1.$$
Сократим константы
$$- \frac{1}{n} \cdot nM \, ln \, X_1 =
  -M \, ln \, X_1.$$
Найдём математическое ожидание логарифма случайной величины выборки
$$M \, ln \, X_1 =
  \int \limits_0^1 ln \, x \cdot \theta x^{ \theta - 1} dx =
  \theta \int \limits_0^1 x^{ \theta - 1} ln \, x dx.$$
Возьмём интеграл по частям
$$u = ln \, x, \,
  dv = x^{ \theta - 1} dx, \,
  du = \frac{dx}{x}, \,
  v = \frac{x^{ \theta }}{ \theta },$$
получим
$$ \theta \int \limits_0^1 x^{ \theta - 1} ln \, x dx =
  \theta \cdot \left. ln \, x \cdot \frac{x^{ \theta }}{ \theta } \right|_0^1 -
  \theta \int \limits_0^1 \frac{x^{ \theta }}{ \theta } \cdot \frac{dx}{x} =
  - \int \limits_0^1 x^{ \theta - 1} dx =
  - \left. \frac{x^{ \theta }}{ \theta } \right|_0^1 =
  - \frac{1}{ \theta }.$$
Запишем через параметр
$$- \frac{1}{ \theta } =
  - \tau.$$
Подставляем найденное значение математического ожидания в математическое ожидание оценки
$-M \, ln \, X_1 =
  - \left( - \tau \right) =
  \tau $,
значит, имеем несмещённость.

Теперь ищем дисперсию
$$D \left( - \overline{ln \, X} \right) =
  \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n \, ln \, X_i =
  \frac{1}{n} \left[ M \left( ln \, X_1 \right)^2 - \left( M \, ln \, X_1 \right)^2 \right].$$
Вычислим первое слагаемое из скобки
$$M \, ln^2 X_1 =
  \int \limits_0^1 ln^2 x \cdot \theta x^{ \theta - 1} dx =
  \theta \int \limits_0^1 x^{ \theta - 1} ln^2 xdx.$$
Возьмём интеграл по частям
$$u = ln^2 x, \,
  dv = x^{ \theta - 1} dx, \,
  du = 2ln \, x \cdot \frac{dx}{x}, \,
  v = \frac{x^{ \theta }}{ \theta }.$$
Получим
$$ \theta \int \limits_0^1 x^{ \theta - 1} ln^2 xdx =
  \theta \cdot \left. ln^2 x \cdot \frac{x^{ \theta }}{ \theta } \right|_0^1 -
  \theta \int \limits_0^1 \frac{x^{ \theta }}{ \theta } \cdot 2ln \, x \cdot \frac{dx}{x}.$$
Первое слагаемое зануляется, во втором сокращаем $ \theta $ и выносим двойку за знак интеграла
$$ \theta \int \limits_0^1 \frac{x^{ \theta }}{ \theta } \cdot 2ln \, x \cdot \frac{dx}{x} =
  -2 \int \limits_0^1 x^{ \theta - 1} ln \, xdx.$$
Возьмём интеграл по частям
$$u = ln \, x, \,
  dv = x^{ \theta - 1} dx, \,
  du = \frac{dx}{x}, \,
  v = \frac{x^{ \theta }}{ \theta }.$$
Получим
$$-2 \int \limits_0^1 x^{ \theta - 1} ln \, xdx =
  -2 \cdot \left. ln \, x \cdot \frac{x^{ \theta }}{ \theta } \right|_0^1 +
  2 \int \limits_0^1 \frac{x^{ \theta }}{ \theta } \cdot \frac{dx}{x} =
  \frac{2}{ \theta } \int \limits_0^1 x^{ \theta - 1} dx =
  \frac{2}{ \theta^2}.$$
Дисперсия оценки равна
$$ \frac{1}{n} \left[ M \left( ln \, X_1 \right)^2 - \left( M \, ln \, X_1 \right)^2 \right] =
  \frac{1}{n} \left( \frac{2}{ \theta^2} - \frac{1}{ \theta^2} \right) =
  \frac{1}{n \theta^2} =
  \frac{ \tau^2}{n}.$$

Ищем количество информации. Начинаем с функции правдоподобия
$$L \left( X_1, \tau \right) =
  \frac{1}{ \tau } \cdot X_1^{ \frac{1}{ \tau - 1}} \cdot
  \mathbbm{1} \left\{ X_1 \in \left( 0, 1 \right) \right\}.$$
Пусть $X_1 \in \left( 0, 1 \right) $.
Тогда берём логарифм
$$ln \, L \left( X_1, \tau \right) =
  -ln \, \tau + \left( \frac{1}{ \tau } - 1 \right) ln \, X_1 =
  -ln \, \tau + \frac{1}{ \tau } \cdot ln \, X_1 - ln \, X_1.$$

Берём первую производную от логарифма функции правдоподобия
$$ \frac{ \partial ln \, L \left( X_1, \tau \right) }{ \partial \tau } =
  - \frac{1}{ \tau } - \frac{1}{ \tau^2} \cdot ln \, X_1.$$

Берём вторую производную от логарифма функции правдоподобия
$$ \frac{ \partial^2 ln \, L \left( X_1, \tau \right) }{ \partial \tau^2} =
  \frac{1}{ \tau^2} + \frac{2}{ \tau^3} \cdot ln \, X_1.$$

Ищем количество информации
$$I \left( \tau \right) =
  -M \left( \frac{1}{ \tau^2} + \frac{2}{ \tau^3} \cdot ln \, X_1 \right) =
  - \frac{1}{ \tau^2} - \frac{2}{ \tau^3} \cdot M \, ln \, X_1 =
  - \frac{1}{ \tau^2} + \frac{2}{ \tau^3} \cdot \tau =
  \frac{1}{ \tau^2}.$$

Ищем обратную величину к количеству информации и делим на $n$.
Получим
$$ \frac{1}{nI \left( \tau \right) } =
  \frac{ \tau^2}{n} =
  D \left( - \overline{ln \, X} \right).$$

Значит, $- \overline{ln \, X}$ --- эффективная оценка.

\subsubsection*{6.13}

\textit{Задание.}
Пусть $X_1, \dotsc, X_n$ ---
выборка из распределения $N \left( \mu, \theta \right) $ с неизвестным параметром $ \theta $
и известным математическим ожиданием.
Докажите, что статистика
$$ \theta^* =
  \frac{1}{n} \sum \limits_{i = 1}^n \left( X_i - \mu \right)^2$$
является эффективной по Рао-Крамеру оценкой параметра $ \theta $.

\textit{Решение.} Проверим несмещённость
$$M \theta^* =
  M \left[ \frac{1}{n} \sum \limits_{i = 1}^n \left( X_i - \mu \right)^2 \right] =
  \frac{1}{n} \cdot M \sum \limits_{i = 1}^n \left( X_i - \mu \right)^2 =
  \frac{1}{n} \cdot nM \left( X_1 - \mu \right)^2.$$
Сократим $n$ и раскроем квадрат
$$ \frac{1}{n} \cdot nM \left( X_1 - \mu \right)^2 =
  M \left( X_1^2 - 2\mu X_1 + \mu^2 \right) =
  MX_1^2 - 2 \mu MX_1 + \mu^2.$$
По свойствам дисперсии
$$MX_1^2 - 2 \mu MX_1 + \mu^2 =
  DX_1 + \left( MX_1 \right)^2 - 2 \mu MX_1 + \mu^2 =
  \theta + \mu^2 - 2 \mu \cdot \mu + \mu^2 =
  \theta.$$

Несмещённость есть.
Ищем дисперсию
$$D \theta^* =
  D \left[ \frac{1}{n} \sum \limits_{i = 1}^n \left( X_i - \mu \right)^2 \right] =
  \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n \left( X_i - \mu \right)^2 =
  \frac{1}{n} \cdot D \left( X_1 - \mu \right)^2.$$
Вычислим отдельно дисперсию
$D \left( X_1 - \mu \right)^2 =
  M \left( X_1 - \mu \right)^4 - \left[ M \left( X_1 - \mu \right)^2 \right]^2$.
Случайная величина $X_1 - \mu $ имеет распределение $N \left( 0, \theta \right) $.
Если X имеет нормальное распределение $N \left( 0, \sigma^2 \right) $,
то для неё существуют (конечные) моменты при всех p с действительной частью больше $-1$.
Для неотрицательных целых p, центральные моменты таковы
$$MX^p =
  \begin{cases}
    0, \qquad p = 2n + 1, \\
    \sigma^p \left( p - 1 \right)!!, \qquad p = 2n.
  \end{cases}$$
В данной задаче $ \sigma^2 = \theta $, следовательно $ \sigma = \sqrt{ \theta }$.
Подставим значения математических ожиданий четвёртого и второго моментов
$$M \left( X_1 - \mu \right)^4 - \left[ M \left( X_1 - \mu \right)^2 \right]^2 =
  \left( \sqrt{ \theta } \right)^4 \left( 4 - 1 \right)!! -
  \left[ \left( \sqrt{ \theta } \right)^2 \left( 2 - 1 \right)!! \right]^2.$$
Упростим
$$ \left( \sqrt{ \theta } \right)^4 \left( 4 - 1 \right)!! -
  \left[ \left( \sqrt{ \theta } \right)^2 \left( 2 - 1 \right)!! \right]^2 =
  3!! \theta^2 - \theta^2 =
  3 \theta^2 - \theta^2 =
  2 \theta^2.$$
Тогда дисперсия оценки равна
$$ \frac{1}{n} \cdot D \left( X_1 - \mu \right)^2 =
  \frac{2 \theta^2}{n}.$$

Переходим к функции правдоподобия и оценке количества информации.
Смотрим на одно наблюдение
$$L \left( X_1, \theta \right) =
  \frac{1}{ \sqrt{2 \pi \theta }} \cdot e^{- \frac{ \left( X_1 - \mu \right)^2}{2 \theta }}.$$

Логарифмируем
$$ln \, L \left( X_1, \theta \right) =
  - \frac{1}{2} \cdot ln \left( 2 \pi \theta \right) -
  \frac{ \left( X_1 - \mu \right)^2}{2 \theta } =
  - \frac{1}{2} \cdot ln \left( 2 \pi \right) - \frac{1}{2} \cdot ln \theta -
  \frac{ \left( X_1 - \mu \right)^2}{2 \theta }.$$

Ищем первую производную
$$ \frac{ \partial \, ln \, L \left( X_1, \theta \right) }{ \partial \theta } =
  - \frac{1}{2 \theta } + \frac{ \left( X_1 -  \mu \right)^2}{2 \theta^2}.$$

Ищем вторую производную
$$ \frac{ \partial^2 ln \, L \left( X_1, \theta \right) }{ \partial \theta^2} =
  \frac{1}{2 \theta^2} - \frac{ \left( X_1 - \mu \right)^2}{ \theta^3}.$$

Таким образом,
$$I \left( \theta \right) =
  -M \left( \frac{1}{2 \theta^2} - \frac{ \left( X_1 - \mu \right)^2}{ \theta^3} \right) =
  - \frac{1}{2 \theta^2} + \frac{1}{ \theta^3} \left( MX_1^2 - 2 \mu MX_1 + \mu^2 \right).$$
По свойствам дисперсии
$$- \frac{1}{2 \theta^2} + \frac{1}{ \theta^3} \left( MX_1^2 - 2 \mu MX_1 + \mu^2 \right) =
  - \frac{1}{2 \theta^2} +
  \frac{1}{ \theta^3} \left( DX_1 + \left( MX_1 \right)^2 - 2 \mu \mu + \mu^2 \right).$$
Подставим значение математического ожидания и дисперсии
$$- \frac{1}{2 \theta^2} +
  \frac{1}{ \theta^3} \left( DX_1 + \left( MX_1 \right)^2 - 2 \mu \mu + \mu^2 \right) =
  - \frac{1}{2 \theta^2} + \frac{1}{ \theta^3} \left( \theta + \mu^2 - 2 \mu^2 + \mu^2 \right).$$
Упростим
$$- \frac{1}{2 \theta^2} + \frac{1}{ \theta^3} \left( \theta + \mu^2 - 2 \mu^2 + \mu^2 \right) =
  - \frac{1}{2 \theta^2} + \frac{1}{ \theta^2} =
  \frac{1}{2 \theta^2}.$$

Формируем величину
$$ \frac{1}{nI \left( \theta \right) } =
  \frac{2 \theta^2}{n} =
  D \theta^*,$$
значит, оценка эффективная.

\subsubsection*{6.14}

\textit{Задание.} Пусть $X_1, \dotsc, X_n$ --- выборка из распределения с плотностью
$$f_{ \theta } \left( x \right) =
  \begin{cases}
    \frac{1}{x \sigma \sqrt{2 \pi }} \cdot
    e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}}, \qquad x \geq 0, \\
    0, \qquad x < 0,
  \end{cases}$$
где $ \sigma $ --- известный параметр.
Убедитесь, что статистика
$$ \theta_n^* =
  \frac{1}{n} \sum \limits_{i = 1}^n ln \, X_i$$
является несмещённой оценкой параметра $ \theta $ и докажите,
что $ \theta_n^*$ является эффективной по Рао-Крамеру оценкой параметра $ \theta $.

\textit{Решение.}
$$M \theta_n^* =
  M \left( \frac{1}{n} \sum \limits_{i = 1}^n ln \, X_i \right) =
  \frac{1}{n} \cdot M \sum \limits_{i = 1}^n ln \, X_i =
  \frac{1}{n} \cdot nM \, ln \, X_1 =
  M \, ln \, X_1.$$
Запишем математическое ожидание случайной величины, имеющей плотность, по определению
$$M \, ln \, X_1 =
  \int \limits_{ \mathbb{R}} ln \, x \cdot p \left( x \right) dx =
  \int \limits_{ \mathbb{R}}
    ln \, x \cdot \frac{1}{x \sigma \sqrt{2 \pi }} \cdot
    e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}} \cdot
    \mathbbm{1} \left\{ x \geq 0 \right\} dx.$$
Вынесем константу из-под знака интеграла и воспользуемся индикатором
$$ \int \limits_{ \mathbb{R}}
    ln \, x \cdot \frac{1}{x \sigma \sqrt{2 \pi }} \cdot
    e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}} \cdot
    \mathbbm{1} \left\{ x \geq 0 \right\} dx =
  \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_0^{+ \infty }
    \frac{ln \, x}{x} \cdot e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}} dx.$$
Внесём $x^{-1}$ под знак дифференциала
$$ \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_0^{+ \infty }
    \frac{ln \, x}{x} \cdot e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}} dx =
  \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_0^{+ \infty}
    ln \, x \cdot e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}}
  d \left( ln \, x \right).$$
Делаем замену $ln \, x = t$.
Получаем
$$ \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_0^{+ \infty}
    ln \, x \cdot e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}}
  d \left( ln \, x \right) =
  \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_{- \infty }^{+ \infty } te^{- \frac{ \left( t - \theta \right)^2}{2 \sigma^2}} dt =
  Mt =
  \theta,$$
значит, имеем несмещённость.

Теперь ищем дисперсию
$$D \theta_n^* =
  D \left( \frac{1}{n} \sum \limits_{i = 1}^n ln \, X_i \right) =
  \frac{1}{n^2} \cdot D \sum \limits_{i = 1}^n ln \, X_i =
  \frac{1}{n} \cdot D \, ln \, X_1.$$
По свойствам дисперсии
$$ \frac{1}{n} \cdot D \, ln \, X_1 =
  \frac{1}{n} \left\{ M \left( ln \, X_1 \right)^2 - \left[ M \, ln \, X_1 \right]^2 \right\}.$$
Найдём отдельно второй момент
$$M \left( ln \, X_1 \right)^2 =
  \int \limits_0^{+ \infty }
    ln^2 x \cdot \frac{1}{x \sigma \sqrt{2 \pi }} \cdot
    e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}} dx.$$
Внесём $x^{-1}$ под знак дифференциала
$$ \int \limits_0^{+ \infty }
    ln^2 x \cdot \frac{1}{x \sigma \sqrt{2 \pi }} \cdot
    e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}} dx =
  \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_0^{+ \infty }
    ln^2 x \cdot e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}}
  d \left( ln \, x \right).$$
Делаем замену как и в предыдущем случае
$$ \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_0^{+ \infty }
    ln^2 x \cdot e^{- \frac{ \left( ln \, x - \theta \right)^2}{2 \sigma^2}}
  d \left( ln \, x \right) =
  \frac{1}{ \sigma \sqrt{2 \pi }} \cdot
  \int \limits_{- \infty }^{+ \infty }
    t^2 e^{- \frac{ \left( t - \theta \right)^2}{2 \sigma^2}} dt =
  \theta^2 + \sigma^2.$$
Подставляем в найденное выражение для дисперсии оценки
$$D \theta_n^* =
  \frac{1}{n} \left( \theta^2 + \sigma^2 - \theta^2 \right) =
  \frac{ \sigma^2}{n}.$$

Ищем количество информации.
Начнём с функции правдоподобия
$$L \left( X_1, \theta \right) =
  \frac{1}{X_1 \sigma \sqrt{2 \pi }} \cdot
  e^{- \frac{ \left( ln \, X_1 - \theta \right)^2}{2 \sigma^2}} \cdot
  \mathbbm{1} \left\{ X_1 \geq 0 \right\}.$$

Пусть $X_1 \geq 0$.
Тогда берём логарифм
$$ln \, L \left( X_1, \theta \right) =
  -ln \left( X_1 \sigma \sqrt{2 \pi } \right) -
  \frac{ \left( ln \, X_1 - \theta \right)^2}{2 \sigma^2}.$$

Берём первую производную от логарифма функции правдоподобия
$$ \frac{ \partial ln \, L \left( X_1, \theta \right) }{ \partial \theta } =
  \frac{2 \left( ln \, X_1 - \theta \right) }{2 \sigma^2} =
  \frac{ln \, X_1 - \theta }{ \sigma^2}.$$

Берём вторую производную от логарифма функции правдоподобия
$$ \frac{ \partial^2 ln \, L \left( X_1, \theta \right) }{ \partial \theta^2} =
  - \frac{1}{ \sigma^2}.$$

Ищем количество информации
$$I \left( \theta \right) =
  -M \left( - \frac{1}{ \sigma^2} \right) =
  \frac{1}{ \sigma^2}.$$

Ищем обратную величину к количеству информации и делим на $n$.
Получаем
$$ \frac{1}{nI \left( \theta \right) } =
  \frac{ \sigma^2}{n} =
  D \theta_n^*,$$
значит,
$$ \theta_n^* =
  \frac{1}{n} \sum \limits_{i = 1}^n ln \, X_i$$
--- эффективная оценка.
